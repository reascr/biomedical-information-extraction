{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10749680,"sourceType":"datasetVersion","datasetId":6666943},{"sourceId":10749735,"sourceType":"datasetVersion","datasetId":6666989},{"sourceId":11194607,"sourceType":"datasetVersion","datasetId":6988818}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport os\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom transformers import BertForTokenClassification, BertTokenizer, AdamW","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:39:47.483174Z","iopub.execute_input":"2025-05-15T12:39:47.483532Z","iopub.status.idle":"2025-05-15T12:39:47.488069Z","shell.execute_reply.started":"2025-05-15T12:39:47.483505Z","shell.execute_reply":"2025-05-15T12:39:47.487257Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/d/andreaschrter/gutbrainie2025/gutbrainie2025\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:31:44.635593Z","iopub.execute_input":"2025-05-15T12:31:44.635890Z","iopub.status.idle":"2025-05-15T12:31:44.639630Z","shell.execute_reply.started":"2025-05-15T12:31:44.635869Z","shell.execute_reply":"2025-05-15T12:31:44.638691Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class AnnotationDataset(Dataset):\n    def __init__(self, root_path, tokenizer=None, split='Train', quality_filter=['platinum_quality', 'gold_quality', 'silver_quality']):\n        self.samples = []\n        annotations_dir = os.path.join(root_path, 'Annotations', split)\n            \n        self.tokenizer = tokenizer\n               \n        if split == 'Train':\n            for quality in quality_filter:  # filter out bronze quality since it contains autogenerated annotations\n                quality_dir = os.path.join(annotations_dir, quality)\n                json_format_dir = os.path.join(quality_dir, 'json_format')\n                if not os.path.exists(json_format_dir):\n                    print(f\"No folder {json_format_dir} was found!\")\n                    continue\n                \n                # append data points (tuple of article identifier and corresponding annotations as a dictionary) to the sample list \n                for file_name in os.listdir(json_format_dir):\n                    if file_name.endswith('.json'):\n                        file_path = os.path.join(json_format_dir, file_name)\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            data = json.load(f)\n                            #self.samples.extend(data.items())  \n                            sorted_items = sorted(data.items(), key=lambda item: item[0])  # sort items by article identifier number\n                            self.samples.extend(sorted_items)\n                          \n        elif split == 'Dev':\n            json_format_dir = os.path.join(annotations_dir, 'json_format')\n            if not os.path.exists(json_format_dir):\n                raise FileNotFoundError(f\"No folder {json_format_dir} was found!\")\n                \n            json_files = [fname for fname in os.listdir(json_format_dir) if fname.endswith('.json')]\n            for json_file in json_files:\n                file_path = os.path.join(json_format_dir, json_file)\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                    sorted_items = sorted(data.items(), key=lambda item: item[0]) \n                    self.samples.extend(sorted_items)        \n        else:\n            raise ValueError(\"Specify a split, must be either 'Train' or 'Dev'!\")\n        \n    def __len__(self):\n        return len(self.samples) \n    \n    def __getitem__(self, idx):\n        return self.samples[idx]  # one data point (=article id) with annotations\n        \n    def plot_abstract_lengths(self):\n        \"\"\"\n        Plots the distribution of tokenized word lengths of abstracts using either whitespace tokenization or BERT tokenization.\n        \"\"\"\n        abstract_lengths = []\n        for article_id, data in self.samples:\n            abstract = data['metadata'].get('abstract', '')\n            \n            if self.tokenizer:  # use BERT tokenizer if its given\n                tokens = self.tokenizer.tokenize(abstract)\n                token_count = len(tokens)\n                abstract_lengths.append(token_count)\n                tokenizer_type = \"BERT Tokenized\"\n            else:  # white space tokenization (just as an overview, baselines use NLTK tokenizer)\n                word_count = len(abstract.split())\n                abstract_lengths.append(word_count)\n                tokenizer_type = \"Whitespace Tokenized\"\n                \n        print(\"Maximum number of tokens per abstract: \", max(abstract_lengths))\n        plt.figure(figsize=(8, 4))\n        plt.hist(abstract_lengths, bins=30, color='#E6E6FA', edgecolor='#D1C8E3')\n        plt.title(f\"Distribution of Abstract Lengths ({tokenizer_type})\", fontsize=14, fontweight='bold')\n        plt.xlabel(\"Token Count\" if self.tokenizer else \"Word Count\", fontsize=12, fontweight='medium')\n        plt.ylabel(\"Frequency\", fontsize=12, fontweight='medium')\n        plt.grid(True, linestyle='--', alpha=0.5)\n        plt.tick_params(axis='both', which='major', labelsize=12, length=6, width=1.2, direction='in', grid_alpha=0.5)\n    \n        plt.tight_layout()\n        plt.show()\n\n    \n    def get_text_data(self):\n        \"\"\"\n        Extracts title and abstract text from the dataset.\n        \"\"\"\n        all_titles = []\n        all_abstracts = []\n        \n        for _, data in self.samples:\n            if 'metadata' in data:\n                if 'title' in data['metadata'] and data['metadata']['title']:\n                    all_titles.append(data['metadata']['title'])\n                if 'abstract' in data['metadata'] and data['metadata']['abstract']:\n                    all_abstracts.append(data['metadata']['abstract'])\n            \n        return \" \".join(all_titles), \" \".join(all_abstracts)\n\n    def build_vocab(self): # important for vocabulary coverage check \n        \"\"\"\n        Tokenizes the dataset and builds a vocabulary.\n        \"\"\"\n        vocab = Counter()\n        all_titles, all_abstracts = self.get_text_data()  # get the raw text\n        \n        # tokenize text (based on whitespace and punctuation)\n        words = re.findall(r'\\b\\w+\\b', all_titles.lower()) + re.findall(r'\\b\\w+\\b', all_abstracts.lower())\n        \n        vocab.update(words)  # count all word occurences\n        return vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:58:52.900681Z","iopub.execute_input":"2025-05-15T12:58:52.900997Z","iopub.status.idle":"2025-05-15T12:58:52.914058Z","shell.execute_reply.started":"2025-05-15T12:58:52.900970Z","shell.execute_reply":"2025-05-15T12:58:52.913210Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def create_label_wordcloud(dataset, entity_type, background_color='white', max_words=200, width=800, height=400):\n    \"\"\"\n    Create and a wordcloud for all entity spans of a given entity type in the NERDataset.\"\"\"\n    # get all spans (entities)\n    spans = []\n    for article_id, data in dataset.samples:\n        text = data['metadata'].get('title', '') + ' ' + data['metadata'].get('abstract', '')\n        for ent in data.get('entities', []):\n            if ent['label'].lower() == entity_type.lower():\n                spans.append(ent['text_span'])\n\n    freqs = Counter(spans)\n    \n    wc = WordCloud(background_color=\"white\",collocations=False, colormap='RdPu_r', max_words=200, \n                   width=7200, height=4800).generate_from_frequencies(freqs)\n    \n    plt.figure(figsize=(12,8), dpi=600)\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.savefig(f\"word_cloud_{entity_type}\", dpi=600)\n    plt.show()\n\ndev_dataset = AnnotationDataset(DATA_DIR, tokenizer=tokenizer, split=\"Dev\")\ncreate_label_wordcloud(dev_dataset, 'human')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T13:13:30.713863Z","iopub.execute_input":"2025-05-15T13:13:30.714193Z"}},"outputs":[],"execution_count":null}]}