{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11194607,"sourceType":"datasetVersion","datasetId":6988818}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_fscore_support\nfrom tqdm import tqdm\nfrom transformers import BertForTokenClassification, BertTokenizer, AdamW, get_scheduler, AutoTokenizer, AutoModel\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom collections import Counter\nimport re\nimport wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-10T13:47:34.661774Z","iopub.execute_input":"2025-04-10T13:47:34.661993Z","iopub.status.idle":"2025-04-10T13:48:00.727777Z","shell.execute_reply.started":"2025-04-10T13:47:34.661972Z","shell.execute_reply":"2025-04-10T13:48:00.727135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T13:48:07.029555Z","iopub.execute_input":"2025-04-10T13:48:07.029900Z","iopub.status.idle":"2025-04-10T13:48:07.175142Z","shell.execute_reply.started":"2025-04-10T13:48:07.029873Z","shell.execute_reply":"2025-04-10T13:48:07.174312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip freeze > requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T13:48:22.110244Z","iopub.execute_input":"2025-04-10T13:48:22.110533Z","iopub.status.idle":"2025-04-10T13:48:24.098762Z","shell.execute_reply.started":"2025-04-10T13:48:22.110508Z","shell.execute_reply":"2025-04-10T13:48:24.097765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/gutbrainie2025/gutbrainie2025\"\nwandb.login(key=\"0fe636b8bf5ddbd71f8f52823cc7c39ce880bc1b\") # change this before making it public","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:46:06.993416Z","iopub.execute_input":"2025-04-05T08:46:06.993803Z","iopub.status.idle":"2025-04-05T08:46:13.455102Z","shell.execute_reply.started":"2025-04-05T08:46:06.993760Z","shell.execute_reply":"2025-04-05T08:46:13.454212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AnnotationDataset(Dataset):\n    def __init__(self, root_path, tokenizer=None, split='Train', quality_filter=['platinum_quality', 'gold_quality', 'silver_quality']):\n        self.samples = []\n        annotations_dir = os.path.join(root_path, 'Annotations', split)\n            \n        self.tokenizer = tokenizer\n               \n        if split == 'Train':\n            for quality in quality_filter:  # filter out bronze quality since it contains autogenerated annotations\n                quality_dir = os.path.join(annotations_dir, quality)\n                json_format_dir = os.path.join(quality_dir, 'json_format')\n                if not os.path.exists(json_format_dir):\n                    print(f\"No folder {json_format_dir} was found!\")\n                    continue\n                \n                # append data points (tuple of article identifier and corresponding annotations as a dictionary) to the sample list \n                for file_name in os.listdir(json_format_dir):\n                    if file_name.endswith('.json'):\n                        file_path = os.path.join(json_format_dir, file_name)\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            data = json.load(f)\n                            #self.samples.extend(data.items())  \n                            sorted_items = sorted(data.items(), key=lambda item: item[0])  # sort items by article identifier number\n                            self.samples.extend(sorted_items)\n                          \n        elif split == 'Dev':\n            json_format_dir = os.path.join(annotations_dir, 'json_format')\n            if not os.path.exists(json_format_dir):\n                raise FileNotFoundError(f\"No folder {json_format_dir} was found!\")\n                \n            json_files = [fname for fname in os.listdir(json_format_dir) if fname.endswith('.json')]\n            for json_file in json_files:\n                file_path = os.path.join(json_format_dir, json_file)\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                    sorted_items = sorted(data.items(), key=lambda item: item[0]) \n                    self.samples.extend(sorted_items)        \n        else:\n            raise ValueError(\"Specify a split, must be either 'Train' or 'Dev'!\")\n        \n    def __len__(self):\n        return len(self.samples) \n    \n    def __getitem__(self, idx):\n        return self.samples[idx]  # one data point (=article id) with annotations\n        \n    def plot_abstract_lengths(self):\n        \"\"\"\n        Plots the distribution of tokenized word lengths of abstracts using either whitespace tokenization or BERT tokenization.\n        \"\"\"\n        abstract_lengths = []\n        for article_id, data in self.samples:\n            abstract = data['metadata'].get('abstract', '')\n            \n            if self.tokenizer:  # use BERT tokenizer if its given\n                tokens = self.tokenizer.tokenize(abstract)\n                token_count = len(tokens)\n                abstract_lengths.append(token_count)\n                tokenizer_type = \"BERT Tokenized\"\n            else:  # white space tokenization (just as an overview, baselines use NLTK tokenizer)\n                word_count = len(abstract.split())\n                abstract_lengths.append(word_count)\n                tokenizer_type = \"Whitespace Tokenized\"\n                \n        print(\"Maximum number of tokens per abstract: \", max(abstract_lengths))\n        plt.figure(figsize=(8, 4))\n        plt.hist(abstract_lengths, bins=30, color='#E6E6FA', edgecolor='#D1C8E3')\n        plt.title(f\"Distribution of Abstract Lengths ({tokenizer_type})\", fontsize=14, fontweight='bold')\n        plt.xlabel(\"Token Count\" if self.tokenizer else \"Word Count\", fontsize=12, fontweight='medium')\n        plt.ylabel(\"Frequency\", fontsize=12, fontweight='medium')\n        plt.grid(True, linestyle='--', alpha=0.5)\n        plt.tick_params(axis='both', which='major', labelsize=12, length=6, width=1.2, direction='in', grid_alpha=0.5)\n    \n        plt.tight_layout()\n        plt.show()\n\n    \n    def get_text_data(self):\n        \"\"\"\n        Extracts title and abstract text from the dataset.\n        \"\"\"\n        all_titles = []\n        all_abstracts = []\n        \n        for _, data in self.samples:\n            if 'metadata' in data:\n                if 'title' in data['metadata'] and data['metadata']['title']:\n                    all_titles.append(data['metadata']['title'])\n                if 'abstract' in data['metadata'] and data['metadata']['abstract']:\n                    all_abstracts.append(data['metadata']['abstract'])\n            \n        return \" \".join(all_titles), \" \".join(all_abstracts)\n\n    def build_vocab(self): # important for vocabulary coverage check \n        \"\"\"\n        Tokenizes the dataset and builds a vocabulary.\n        \"\"\"\n        vocab = Counter()\n        all_titles, all_abstracts = self.get_text_data()  # get the raw text\n        \n        # tokenize text (based on whitespace and punctuation)\n        words = re.findall(r'\\b\\w+\\b', all_titles.lower()) + re.findall(r'\\b\\w+\\b', all_abstracts.lower())\n        \n        vocab.update(words)  # count all word occurences\n        return vocab\n\n\ndef split_datasets(train_data, val_data, test_data):\n    '''\n    Splits the training dataset into a new training and validation set.  \n    The validation set is sized based on the test set length.  \n    '''\n    train_size = len(train_data.samples)\n    test_size = len(test_data.samples) \n    val_size = test_size  \n    train_new_size = train_size - val_size  # remaining size for new training set\n\n    train_subset, val_subset = random_split(train_data.samples, [train_new_size, val_size]) # split the train set into new train and val\n\n    train_data.samples = train_subset\n    val_data.samples = val_subset\n\n    return train_data, val_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:46:22.771726Z","iopub.execute_input":"2025-04-05T08:46:22.772022Z","iopub.status.idle":"2025-04-05T08:46:22.787513Z","shell.execute_reply.started":"2025-04-05T08:46:22.772000Z","shell.execute_reply":"2025-04-05T08:46:22.786505Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### RE Dataset for binary tag based relation extraction\n\nDifferently to the NER data set, title and abstract have to be processed together since an entity in the title can be in a relationship with an entity mentioned in the abtract. Further, this increases the need for striding. Moreover, relationships are directional, there is an entity 1 serving as a subject and entity 2 serving as a subject, with entity 2 potentially preceeding entity 1 in the text. This complicates the enumeration of candidate entity pairs, as the number of possible combinations grows factorially.\n\n**Considerations:**\n\n    - If there are n relationships, create n negative pairs (either randomly or with a heuristic filter).\n    - Smart negative sampling: Generate negatives where Entity1 and Entity2 are of the same type as positive samples. Select entities that co-occur in the same document but are far apart (maybe 50%, but this can be done randomly). easy (just in same document), medium (same entity type), and hard (distance based). Also, take into account that a subject can appear after an object (passive structures etc.).\n    - Take all relationships as data points (but consider dummification/replacement with tag)? For now, focus on mention based (only add entity markers, see Gu et al. (2021))\n    - special tokens have to be added to tokenizer's vocabulary\n    - one data point = title + abstract, marked with entity 1 (subject) and entity 2 (object), labels = set(0,1)\n    - Relation encoding: 1) entity 1 + entity 2 concatenated, 2) CLS, 3) CLS + entity 1 + entitiy 2\n    - striding? Or is there any way to use PubMedBERT with longer contexts than 512 tokens? Take into account however, that striding misses relations that are in two separate chunks. This is not an issue for NER, but for RE.\n    - This time take the training loop from the GutBrainie challenge\n    - data augmentation from external resources is more complicated here than for NER\n    - ca. 32.720 training data points if every article ID has 20 relations (and 20 negative relations to balance classes)\n    - for training: using the ground truth NEs probably is fine.\n    - for inference: candidate generation: probably with own NER model. Make an experiment, A) own NER model, B) ground truth. Return set of relations with tags (tag before the mention based NEs.)\n    - apply self attention on concatenation of entity 1 and entity 2\n    - entity aware attention through attention biasing\n    - capture global context better through an additional attention layer\n\n    Gu et al. (2021): For featurization, the relation instance is either represented by a special [CLS] token or by concatenating the mention representations. In the latter case, if an entity mention contains multiple tokens, its representation is usually produced by pooling those of individual tokens (max or average).","metadata":{}},{"cell_type":"code","source":"# more strategic negative sampling (=harder negatives)\n\n# helper functions for modularity of code\ndef get_adjusted_indices(entity, offset):\n    \"\"\"\n    Adjusts start and end indices for entities in the abstract (adds an offset = length of title to abstracts).\n    \"\"\"\n    if entity.get(\"location\") == \"abstract\":\n        start_idx = entity[\"start_idx\"] + offset\n        end_idx = entity[\"end_idx\"] + 1 + offset\n    else:\n        start_idx = entity[\"start_idx\"]\n        end_idx = entity[\"end_idx\"] + 1\n    return start_idx, end_idx\n\n\ndef mark_entities(text, subj_start_idx, subj_end_idx, obj_start_idx, obj_end_idx):\n    \"\"\"\n    Inserts special entity markers into the text.\n    \"\"\"\n    text_chars = list(text)\n    text_chars.insert(obj_end_idx, \"</ent2>\")\n    text_chars.insert(obj_start_idx, \"<ent2>\")\n    text_chars.insert(subj_end_idx, \"</ent1>\")\n    text_chars.insert(subj_start_idx, \"<ent1>\")\n    return \"\".join(text_chars)\n\n\nclass REDataset(AnnotationDataset):\n    def __init__(self, root_path, tokenizer, max_length=512, split=\"Train\", quality_filter=['platinum_quality', 'gold_quality', 'silver_quality']):\n        \"\"\"\n        Creates a relation extraction dataset.\n        Each data point is a concatenation of abstract and title. Entity markers (<ent1> and <ent2>) are inserted to mark the two entities in question.\n        These entity markers have to be added to the tokenizer that is passed to the initialisation of the class.\n        For each article, positive relation candidates are generated based on the ground truth. For this, all possible mention based relations are considered.\n        (A set of tag based relations is later inferred during inference. Here, the entities are not modified except for a special entity marker).\n        Negative samples are generated from other candidate entity pairs (randomly, check later to include easy, medium, and hard examples, especially entities that are the same ones as in the relations).\n        \"\"\"\n        super().__init__(root_path, tokenizer=tokenizer, split=split, quality_filter=quality_filter)\n        #self.tokenizer = tokenizer # is already initiated by the parent class Annotation Data set\n        self.max_length = max_length\n        self.relation_samples = []\n\n        #counter = 0\n        # concatenate title and abstract because a relation can hold between an entity in the title and one in the abstract\n        for article_id, data in self.samples:\n            #counter += 1\n            title = data['metadata'].get('title', '')\n            abstract = data['metadata'].get('abstract', '')\n            full_text = (title + \" \" + abstract).strip() \n            if not full_text:\n                continue\n            \n            # get ground truth entities and all relations \n            entities = data.get(\"entities\", [])\n            relations = data.get(\"relations\", [])\n\n            # for entities in the abstract, we need to add the length of the title to the indices (since we are concatenating titles and abstracts)\n            offset = len(title) + 1\n\n            reversed_pairs = [] # we need this for sampling of negatives below\n            \n            for rel in relations:\n                subj_entity = {\"start_idx\": rel[\"subject_start_idx\"], \"end_idx\": rel[\"subject_end_idx\"], \"location\": rel[\"subject_location\"]}\n                subj_start_idx, subj_end_idx = get_adjusted_indices(subj_entity, offset) # adjust indices for subject\n    \n                obj_entity = {\"start_idx\": rel[\"object_start_idx\"], \"end_idx\": rel[\"object_end_idx\"], \"location\": rel[\"object_location\"]}\n                obj_start_idx, obj_end_idx = get_adjusted_indices(obj_entity, offset) # adjust indices for object\n\n                #if obj_entity != subj_entity: # check whether that is possible\n                #    reversed_pairs.append((obj_entity, subj_entity)) # reversed pairs\n                #    if counter <2:\n                #        print(obj_entity, subj_entity)\n\n                marked_text = mark_entities(full_text, subj_start_idx, subj_end_idx, obj_start_idx, obj_end_idx)\n                    \n                self.relation_samples.append({\n                    \"article_id\": article_id,\n                    \"text\": marked_text,\n                    \"label\": 1 # positive relation\n                })\n\n            # create negative examples by generating as many negative examples as positives (to balance classes)\n            num_pos = len(relations)\n            num_reversed = int(num_pos*0.25)\n            num_same_tag = int(num_pos*0.25)\n            num_random = num_pos - num_reversed - num_same_tag\n\n            #if counter < 2:\n             #   print(\"num pos: \", num_pos, \" num_reversed: \", num_reversed, \" num_random: \", num_random )\n                \n            candidate_pairs = [] # get all possible candidate pairs\n            for i in range(len(entities)):\n                for j in range(len(entities)):\n                    candidate_pairs.append((entities[i], entities[j])) # entities look like this: {'start_idx': 0, 'end_idx': 26, 'location': 'title', 'text_span': 'Lactobacillus fermentum NS9', 'label': 'dietary supplement'}\n            random.shuffle(candidate_pairs)\n            \n            same_tag_pairs = []\n            random_pairs = []\n            \n            for subj, obj in candidate_pairs:\n                # exclude all possible candidate pairs (order matters because we have directional relationships between subj and obj)\n                is_positive = any(subj[\"text_span\"] == r[\"subject_text_span\"] and obj[\"text_span\"] == r[\"object_text_span\"] for r in relations)\n                # exclude all pertubations of positive pairs (we subsample them above)\n                is_reversed = any(obj[\"text_span\"] == r[\"subject_text_span\"] and subj[\"text_span\"] == r[\"object_text_span\"] for r in relations)\n                has_same_tag = any(subj[\"label\"] == r[\"subject_label\"] and obj[\"label\"] == r[\"object_label\"] for r in relations)\n                \n                if not is_positive and not is_reversed:\n                    if has_same_tag:\n                        same_tag_pairs.append((subj,obj))\n                    else:\n                        random_pairs.append((subj,obj))\n\n            # create random subset of the negatives\n            random.shuffle(same_tag_pairs)\n            random.shuffle(random_pairs)\n            random.shuffle(reversed_pairs)\n\n            #if counter < 2:\n             #   print(\"BEFORE SAMPLING: num_reversed: \", len(reversed_pairs), \" num_random: \", len(random_pairs), \"num_same_tag:\", len(same_tag_pairs))\n            \n            sampled_reversed = reversed_pairs[:min(num_reversed, len(reversed_pairs))]\n            sampled_same_tag = same_tag_pairs[:min(num_same_tag, len(same_tag_pairs))]\n            sampled_random = random_pairs[:num_pos - len(sampled_reversed) - len(sampled_same_tag)]\n\n            #if counter < 2:\n             #   print(\"AFTER SAMPLING: num_reversed: \", len(sampled_reversed), \" num_random: \", len(sampled_random), \"num_same_tag:\", len(sampled_same_tag))\n\n            final_negative_pairs = sampled_reversed + sampled_same_tag + sampled_random\n\n            for subj, obj in final_negative_pairs:\n                    subj_start_idx, subj_end_idx = get_adjusted_indices(subj, offset)\n                    obj_start_idx, obj_end_idx = get_adjusted_indices(obj, offset)\n                    marked_text = mark_entities(full_text, subj_start_idx, subj_end_idx, obj_start_idx, obj_end_idx)\n                    self.relation_samples.append({\n                        \"article_id\": article_id,\n                        \"text\": marked_text,\n                        \"label\": 0\n                    }) # might store type of negative here for further examination\n            #if counter < 2:\n                #print(\"Final samples of this abstract:\", len(self.relation_samples))\n        #print(\"Final len:\", len(self.relation_samples))\n        #random.shuffle(self.relation_samples)\n        #self.relation_samples = self.relation_samples[:100]\n    \n    def __len__(self):\n        return len(self.relation_samples)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Returns a tokenized relation extraction data point:\n            - input_ids\n            - attention_mask\n            - label (0 or 1) for binary classification\n\n        Uses a dynamic window approach to make sure entities are captured within the 512 token limit.\n        \"\"\"\n        sample = self.relation_samples[idx]\n        \n        tokenized_text = self.tokenizer(\n            sample[\"text\"], \n            #padding=\"max_length\", \n            truncation=True, \n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n    \n        for key in tokenized_text:\n            tokenized_text[key] = tokenized_text[key].squeeze(0) # ???\n        return {\n            \"input_ids\": tokenized_text[\"input_ids\"],\n            \"attention_mask\": tokenized_text[\"attention_mask\"],\n            \"label\": torch.tensor(sample[\"label\"], dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:07:18.717956Z","iopub.execute_input":"2025-04-05T09:07:18.718274Z","iopub.status.idle":"2025-04-05T09:07:18.733756Z","shell.execute_reply.started":"2025-04-05T09:07:18.718251Z","shell.execute_reply":"2025-04-05T09:07:18.732825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# helper functions for modularity of code\n'''def get_adjusted_indices(entity, offset):\n    \"\"\"\n    Adjusts start and end indices for entities in the abstract (adds an offset = length of title to abstracts).\n    \"\"\"\n    if entity.get(\"location\") == \"abstract\":\n        start_idx = entity[\"start_idx\"] + offset\n        end_idx = entity[\"end_idx\"] + 1 + offset\n    else:\n        start_idx = entity[\"start_idx\"]\n        end_idx = entity[\"end_idx\"] + 1\n    return start_idx, end_idx\n\n\ndef mark_entities(text, subj_start_idx, subj_end_idx, obj_start_idx, obj_end_idx):\n    \"\"\"\n    Inserts special entity markers into the text.\n    \"\"\"\n    text_chars = list(text)\n    text_chars.insert(obj_end_idx, \"</ent2>\")\n    text_chars.insert(obj_start_idx, \"<ent2>\")\n    text_chars.insert(subj_end_idx, \"</ent1>\")\n    text_chars.insert(subj_start_idx, \"<ent1>\")\n    return \"\".join(text_chars)\n\n\nclass REDataset(AnnotationDataset):\n    def __init__(self, root_path, tokenizer, max_length=512, split=\"Train\", quality_filter=['platinum_quality', 'gold_quality', 'silver_quality']):\n        \"\"\"\n        Creates a relation extraction dataset.\n        Each data point is a concatenation of abstract and title. Entity markers (<ent1> and <ent2>) are inserted to mark the two entities in question.\n        These entity markers have to be added to the tokenizer that is passed to the initialisation of the class.\n        For each article, positive relation candidates are generated based on the ground truth. For this, all possible mention based relations are considered.\n        (A set of tag based relations is later inferred during inference. Here, the entities are not modified except for a special entity marker).\n        Negative samples are generated from other candidate entity pairs (randomly, check later to include easy, medium, and hard examples, especially entities that are the same ones as in the relations).\n        \"\"\"\n        super().__init__(root_path, tokenizer=tokenizer, split=split, quality_filter=quality_filter)\n        #self.tokenizer = tokenizer # is already initiated by the parent class Annotation Data set\n        self.max_length = max_length\n        self.relation_samples = []\n\n        counter = 0\n        \n        # concatenate title and abstract because a relation can hold between an entity in the title and one in the abstract\n        for article_id, data in self.samples:\n            counter += 1\n            title = data['metadata'].get('title', '')\n            abstract = data['metadata'].get('abstract', '')\n            full_text = (title + \" \" + abstract).strip() \n            if not full_text:\n                continue\n            \n            # get ground truth entities and all relations \n            entities = data.get(\"entities\", [])\n            relations = data.get(\"relations\", [])\n\n            # for entities in the abstract, we need to add the length of the title to the indices (since we are concatenating titles and abstracts)\n            offset = len(title) + 1\n\n            for rel in relations:\n                subj_entity = {\"start_idx\": rel[\"subject_start_idx\"], \"end_idx\": rel[\"subject_end_idx\"], \"location\": rel[\"subject_location\"]}\n                subj_start_idx, subj_end_idx = get_adjusted_indices(subj_entity, offset) # adjust indices for subject\n    \n                obj_entity = {\"start_idx\": rel[\"object_start_idx\"], \"end_idx\": rel[\"object_end_idx\"], \"location\": rel[\"object_location\"]}\n                obj_start_idx, obj_end_idx = get_adjusted_indices(obj_entity, offset) # adjust indices for object\n\n                marked_text = mark_entities(full_text, subj_start_idx, subj_end_idx, obj_start_idx, obj_end_idx)\n                    \n                self.relation_samples.append({\n                    \"article_id\": article_id,\n                    \"text\": marked_text,\n                    \"label\": 1 # positive relation\n                })\n\n            \n            # create negative examples by generating as many negative examples as positives (to balance classes)\n            num_pos = len(relations)\n            #if counter < 2:\n                #print(num_pos)\n\n            candidate_pairs = [] # get all possible candidate pairs\n            for i in range(len(entities)):\n                for j in range(len(entities)):\n                    candidate_pairs.append((entities[i], entities[j])) # entities look like this: {'start_idx': 0, 'end_idx': 26, 'location': 'title', 'text_span': 'Lactobacillus fermentum NS9', 'label': 'dietary supplement'}\n            \n            random.shuffle(candidate_pairs)\n            negatives_added = 0\n            # here we can calculate a percentage... if negatives_added/num_pos < 0.5... and then have 50% easy, 25% medium, 25% hard\n            for pair in candidate_pairs:\n                subj, obj = pair\n                # exclude all possible candidate pairs (order matters because we have directional relationships between subj and obj)\n                is_positive = any(subj[\"text_span\"] == r[\"subject_text_span\"] and obj[\"text_span\"] == r[\"object_text_span\"] for r in relations)\n                #if counter < 2 and is_positive:\n                    #print(\"is positive: \", subj, obj, \"\\n\")\n                if not is_positive:\n                    #if counter <2:\n                        #print(\"negative\", subj, obj)\n                    # add the negative example\n                    subj_start_idx, subj_end_idx = get_adjusted_indices(subj, offset)\n                    obj_start_idx, obj_end_idx = get_adjusted_indices(obj, offset)\n\n                    marked_text = mark_entities(full_text, subj_start_idx, subj_end_idx, obj_start_idx, obj_end_idx)\n                    \n                    self.relation_samples.append({\n                        \"article_id\": article_id,\n                        \"text\": marked_text,\n                        \"label\": 0\n                    })\n                    \n                    negatives_added += 1\n                    if negatives_added >= num_pos:\n                        break # we want a balanced data set, stop if number of positives is reached\n            \n                        \n        #random.shuffle(self.relation_samples)\n        #self.relation_samples = self.relation_samples[:100]\n    \n    def __len__(self):\n        return len(self.relation_samples)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Returns a tokenized relation extraction data point:\n            - input_ids\n            - attention_mask\n            - label (0 or 1) for binary classification\n\n        Uses a dynamic window approach to make sure entities are captured within the 512 token limit.\n        \"\"\"\n        sample = self.relation_samples[idx]\n        \n        tokenized_text = self.tokenizer(\n            sample[\"text\"], \n            #padding=\"max_length\", \n            truncation=True, \n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n    \n        for key in tokenized_text:\n            tokenized_text[key] = tokenized_text[key].squeeze(0) # ???\n        return {\n            \"input_ids\": tokenized_text[\"input_ids\"],\n            \"attention_mask\": tokenized_text[\"attention_mask\"],\n            \"label\": torch.tensor(sample[\"label\"], dtype=torch.long)\n        }'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Sanity checks","metadata":{}},{"cell_type":"code","source":"tokenizer = \"NeuML/pubmedbert-base-embeddings\"\ntokenizer = AutoTokenizer.from_pretrained(tokenizer) \nspecial_tokens = ['<ent1>', '</ent1>', '<ent2>', '</ent2>']\ntest_dataset = REDataset(DATA_DIR, tokenizer=tokenizer, split=\"Dev\")\ntest_dataset.tokenizer.add_tokens(special_tokens) # make sure to also resize the size of embeddings later for the model\nprint(len(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:07:21.197003Z","iopub.execute_input":"2025-04-05T09:07:21.197346Z","iopub.status.idle":"2025-04-05T09:07:21.727384Z","shell.execute_reply.started":"2025-04-05T09:07:21.197300Z","shell.execute_reply":"2025-04-05T09:07:21.726516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"index = 4\ninput_ids = test_dataset[index]['input_ids']\n\ndecoded_text = test_dataset.tokenizer.decode(input_ids, skip_special_tokens=True)\n\ntokens = test_dataset.tokenizer.convert_ids_to_tokens(input_ids)\n\nprint(\"Decoded text:\", decoded_text)\nprint(\"Tokens:\", tokens)\nprint(test_dataset[index])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:57:10.017772Z","iopub.execute_input":"2025-04-05T08:57:10.018181Z","iopub.status.idle":"2025-04-05T08:57:10.034203Z","shell.execute_reply.started":"2025-04-05T08:57:10.018144Z","shell.execute_reply":"2025-04-05T08:57:10.033376Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prepare data loaders","metadata":{}},{"cell_type":"code","source":"# https://pytorch.org/docs/stable/data.html\n\ndef collate_fn(batch):\n    input_ids = [item[\"input_ids\"] for item in batch]\n    attention_masks = [item[\"attention_mask\"] for item in batch]\n    labels = torch.stack([item[\"label\"] for item in batch])\n    \n    # dynamic padding to longest seq of batch (to increase computational efficiency)\n    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0) # tokenizer.pad_token_id\n    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0) # https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\n    # keep batch size as first dimension. Tensor of size B x T x * where T is the length of the longest sequence\n    \n    return {\n        \"input_ids\": input_ids_padded,\n        \"attention_mask\": attention_masks_padded,\n        \"label\": labels\n    }\n\ndef create_dataloaders(batch_size, tokenizer, device):\n    train_dataset = REDataset(DATA_DIR, tokenizer=tokenizer, split=\"Train\")\n    val_dataset = REDataset(DATA_DIR, tokenizer=tokenizer, split=\"Train\")  # dummy val data set\n    test_dataset = REDataset(DATA_DIR, tokenizer=tokenizer, split=\"Dev\")  # take dev set as test set (until official test set release)\n\n    # split into train and val \n    train_dataset, val_dataset = split_datasets(train_dataset, val_dataset, test_dataset)\n    #print(train_dataset[1])\n    \n    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=collate_fn)\n    val_dataloader = DataLoader(val_dataset, batch_size, shuffle=True, collate_fn=collate_fn)\n    test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=collate_fn)\n    \n    return train_dataloader, val_dataloader, test_dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:44:39.403023Z","iopub.execute_input":"2025-04-04T06:44:39.403342Z","iopub.status.idle":"2025-04-04T06:44:39.410601Z","shell.execute_reply.started":"2025-04-04T06:44:39.403319Z","shell.execute_reply":"2025-04-04T06:44:39.409584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = REDataset(DATA_DIR, tokenizer=tokenizer, split=\"Train\")\nval_dataset = REDataset(DATA_DIR, tokenizer=tokenizer, split=\"Train\")  # dummy val data set\ntest_dataset = REDataset(DATA_DIR, tokenizer=tokenizer, split=\"Dev\")  # take test as val \n\n# split into train and val \ntrain_dataset, val_dataset = split_datasets(train_dataset, val_dataset, test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:44:41.236589Z","iopub.execute_input":"2025-04-04T06:44:41.237060Z","iopub.status.idle":"2025-04-04T06:44:45.174953Z","shell.execute_reply.started":"2025-04-04T06:44:41.237008Z","shell.execute_reply":"2025-04-04T06:44:45.174055Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Architecture\n\n","metadata":{}},{"cell_type":"code","source":"class RelationClassifier(nn.Module):\n    def __init__(self, model, hidden_size, dropout, ent1_start_id, ent1_end_id, ent2_start_id, ent2_end_id):\n        \"\"\"\n        Binary relation classification model using a BERT-based model with a linear classification layer on top.\n        The hidden size should reflect the adjusted embedding size of the model after adding special tokens to the tokenizer.\n        \"\"\"\n\n        super(RelationClassifier, self).__init__()\n        self.transformer = model\n        self.dropout = nn.Dropout(dropout)\n        self.hidden_size = hidden_size\n        #self.attention = nn.MultiheadAttention(embed_dim=hidden_size * 3, num_heads=8)\n        self.classifier = nn.Linear(hidden_size * 3, 1) # input is concatenation of CLS + ent1 + ent2, output is one logit (binary classification)\n\n        self.ent1_start_id = ent1_start_id\n        self.ent1_end_id = ent1_end_id\n        self.ent2_start_id = ent2_start_id\n        self.ent2_end_id = ent2_end_id\n        \n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask) # get model outputs\n        sequence_output = outputs.last_hidden_state  # take the last hidden state, shape: (batch_size, seq_len, hidden_size) \n        cls_repr = sequence_output[:, 0, :] # CLS token (first token in each sequence)\n        \n        batch_size = input_ids.size(0)\n        ent1_repr_list = [] # store entity representations (one per sequence), list has length = batch size\n        ent2_repr_list = [] \n        \n        # iterate over sequences in a batch\n        for i in range(batch_size):\n            tokens = input_ids[i]  # shape: (seq_len)\n            token_reps = sequence_output[i] # shape: (seq_len, hidden_size)\n            \n            # get the start and end of entities by the entity markers in input ids. The .nonzero() function finds the positions in the tensor where these tokens occur.\n            ent1_start_pos = (tokens == self.ent1_start_id).nonzero(as_tuple=True)[0] # as_tuple=True to get 1D tensor. [0] takes the first occurrence of start token (since there is only one)\n            ent1_end_pos = (tokens == self.ent1_end_id).nonzero(as_tuple=True)[0]\n            ent2_start_pos = (tokens == self.ent2_start_id).nonzero(as_tuple=True)[0]\n            ent2_end_pos = (tokens == self.ent2_end_id).nonzero(as_tuple=True)[0]\n            \n            # extract entity representations and average over subtokens\n            if len(ent1_start_pos) > 0 and len(ent1_end_pos) > 0:\n                start_idx = ent1_start_pos[0].item() + 1 #  convert 1D tensor to integer and exluce the ent marker pos (just take the average of tokens between them)\n                end_idx = ent1_end_pos[0].item()\n                ent1_repr = token_reps[start_idx:end_idx].mean(dim=0) # average over subtokens \n            else:\n                ent1_repr = torch.zeros(token_reps.size(1), device=token_reps.device) # otherwise return 0 vector (but after dynamic padding it shouldnt be a problem anymore)\n            \n            if len(ent2_start_pos) > 0 and len(ent2_end_pos) > 0:\n                start_idx = ent2_start_pos[0].item() + 1\n                end_idx = ent2_end_pos[0].item()\n                ent2_repr = token_reps[start_idx:end_idx].mean(dim=0)  # average over subtokens \n            else:\n                ent2_repr = torch.zeros(token_reps.size(1), device=token_reps.device)\n                \n            ent1_repr_list.append(ent1_repr) # add the entity representation for this sequence to the list for the whole batch\n            ent2_repr_list.append(ent2_repr)\n        \n        ent1_repr = torch.stack(ent1_repr_list, dim=0) # stack the 16 (=batch size) tensors (with shape=hidden_size) along batch dimension =  --> shape = (batch_size,hidden_dim)\n        ent2_repr = torch.stack(ent2_repr_list, dim=0)\n        \n        # concatentate CLS, entity1, and entity2 representations\n        combined_repr = torch.cat([cls_repr, ent1_repr, ent2_repr], dim=1) # concatenate CLS token + entity representations along second dimension --> shape = (batch_size, hidden_dim*3) \n        combined_repr = self.dropout(combined_repr) # add a dropout layer\n        \n        # single logit as output (for binary classification)\n        logit = self.classifier(combined_repr).squeeze(1)  # shape: (batch_size). We want only one logit per seq\n\n        return logit ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:45:19.044460Z","iopub.execute_input":"2025-04-04T06:45:19.044755Z","iopub.status.idle":"2025-04-04T06:45:19.054522Z","shell.execute_reply.started":"2025-04-04T06:45:19.044733Z","shell.execute_reply":"2025-04-04T06:45:19.053449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# seed function taken from https://github.com/heraclex12/R-BERT-Relation-Classification/blob/master/BERT_for_Relation_Classification.ipynb\ndef set_seed(seed):\n    \"\"\"Sets a random seed.\"\"\"\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef train_and_evaluate(model_name, tokenizer_voc_size, seed, train_dataloader, val_dataloader, test_dataloader, lr, weight_decay, num_epochs, dropout, device, max_norm, ent1_start_id, ent1_end_id, ent2_start_id, ent2_end_id, threshold):\n    \"\"\"\n    Trains and evaluates a relation classification model.\n    \"\"\"\n    set_seed(seed)\n    \n    model_str = MODEL_CONFIGS[model_name][\"model_name\"]\n    model = AutoModel.from_pretrained(model_str)\n    model.resize_token_embeddings(tokenizer_voc_size)  # adjust embeddings of the model for special tokens\n\n    hidden_size = model.config.hidden_size\n    \n    model = RelationClassifier(model, hidden_size, dropout, ent1_start_id, ent1_end_id, ent2_start_id, ent2_end_id).to(device)\n\n    bce_loss = nn.BCEWithLogitsLoss()  # use binary crossentropy loss, cf. https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html (combines a Sigmoid layer and BCELoss).\n    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    num_training_steps = len(train_dataloader) * num_epochs # this will be displayed in wandb on the x-axis\n    num_warmup_steps = int(0.1 * num_training_steps) # 10% warm up steps\n    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n\n    best_val_loss = float('inf')\n    patience = 1 # 1 epoch patience\n    patience_counter = 0\n\n    train_losses, val_losses = [], []\n    train_f1s_micro, val_f1s_micro = [], []\n    train_f1s_macro, val_f1s_macro = [], []\n\n    global_step = 0\n    global_step_val = 0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        total_train_loss = 0\n        all_train_labels, all_train_preds = [], []\n\n        # cf. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n        for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device).float()\n\n            optimizer.zero_grad() # zero parameter gradients\n            \n            outputs = model(input_ids, attention_mask) # forward pass\n            loss = bce_loss(outputs, labels) # calculate bce loss\n\n            loss.backward() # backward pass \n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n            optimizer.step() # optimize\n            scheduler.step()\n\n            total_train_loss += loss.item()\n            \n            preds = (torch.sigmoid(outputs).detach().cpu().numpy() > threshold).astype(int) \n            all_train_labels.extend(labels.cpu().numpy())\n            all_train_preds.extend(preds)\n\n            batch_f1_micro = f1_score(labels.cpu().numpy(), preds, average=\"micro\") # calculate micro F1\n            batch_f1_macro = f1_score(labels.cpu().numpy(), preds, average=\"macro\") # calculate macro F1\n\n            # log the micro F1 and macro F1 into wandb\n            wandb.log({\n                \"step\": global_step,\n                \"train_loss_batch\": loss.item(),\n                \"train_f1_micro_batch\": batch_f1_micro,\n                \"train_f1_macro_batch\": batch_f1_macro,\n            })\n            global_step += 1\n\n        train_losses.append(total_train_loss / len(train_dataloader))\n        train_f1_micro = f1_score(all_train_labels, all_train_preds, average=\"micro\")\n        train_f1_macro = f1_score(all_train_labels, all_train_preds, average=\"macro\")\n        train_f1s_micro.append(train_f1_micro)\n        train_f1s_macro.append(train_f1_macro)\n\n        wandb.log({\n        \"train_loss\": train_losses[-1],  \n        \"train_f1_micro\": train_f1s_micro[-1],\n        \"train_f1_macro\": train_f1s_macro[-1],})\n        #}, step=epoch + 1) # log per epoch not step\n\n        model.eval()\n        total_val_loss = 0\n        all_val_labels, all_val_preds = [], []\n\n        with torch.no_grad():\n            for batch in tqdm(val_dataloader, desc=f\"Validation\", leave=False):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['label'].to(device).float()\n\n                outputs = model(input_ids, attention_mask)\n                loss = bce_loss(outputs, labels)\n\n                total_val_loss += loss.item()\n                all_val_labels.extend(labels.cpu().numpy())\n                all_val_preds.extend(torch.sigmoid(outputs).cpu().numpy() > threshold)\n\n                global_step_val += 1\n\n        val_losses.append(total_val_loss / len(val_dataloader))\n        val_f1_micro = f1_score(all_val_labels, all_val_preds, average=\"micro\")\n        val_f1_macro = f1_score(all_val_labels, all_val_preds, average=\"macro\")\n        val_f1s_micro.append(val_f1_micro)\n        val_f1s_macro.append(val_f1_macro)\n\n        wandb.log({\n        \"step\": global_step_val,\n        \"val_loss\": val_losses[-1],  \n        \"val_f1_micro\": val_f1s_micro[-1],\n        \"val_f1_macro\": val_f1s_macro[-1],\n        }) \n\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n        print(f\"Training Loss: {train_losses[-1]:.3f}, Train F1 macro: {train_f1s_macro[-1]:.3f},Train F1 micro: {train_f1s_micro[-1]:.3f} \")\n        print(f\"Validation Loss: {val_losses[-1]:.3f}, Val F1 macro: {val_f1s_macro[-1]:.3f}, Val F1 micro: {val_f1s_micro[-1]:.3f} \")\n\n        # early stopping should be triggered if loss is not decreasing\n        if val_losses[-1] < best_val_loss:\n            best_val_loss = val_losses[-1]\n            patience_counter = 0\n            best_model_state = model.state_dict()\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Stopping early, no improvement in decreasing loss!\")\n                break\n\n    # load the best model (with lowest loss)\n    model.load_state_dict(best_model_state)\n    model.eval() # set to evaluation mode\n\n    # evalzte on test set\n    all_test_labels, all_test_preds = [], []\n    with torch.no_grad():\n        for batch in tqdm(test_dataloader, desc=\"Test Evaluation\", leave=False):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device).float()\n\n            logits = model(input_ids, attention_mask)\n            all_test_labels.extend(labels.cpu().numpy())\n            all_test_preds.extend(torch.sigmoid(logits).cpu().numpy() > threshold)\n\n    test_f1_micro = f1_score(all_test_labels, all_test_preds, average='micro')\n    test_f1_macro = f1_score(all_test_labels, all_test_preds, average='macro')\n\n    print(f\"Test micro f1: {test_f1_micro:.4f}\")\n    print(f\"Test macro f1: {test_f1_macro:.4f}\")\n\n    wandb.log({\n        \"test_f1_micro\": test_f1_micro,\n        \"test_f1_macro\": test_f1_macro,\n    })\n\n    wandb.finish()\n\n    return model, test_f1_micro, test_f1_macro, train_losses, val_losses, train_f1s_micro, train_f1s_macro, val_f1s_micro, val_f1_macro\n\n# modified after Chollet 2018: 75 ADD REFERENCE\ndef plot_train_val_metrics(train_losses, val_losses, train_f1s, val_f1s, model_name, seed):\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n    axs[0].plot(train_losses, label=f'Training loss', color=\"cyan\")\n    axs[0].plot(val_losses, label=f'Validation loss', linestyle='--', color=\"magenta\")\n    #axs[0].set_title(f'{model_name} - Train and Val Loss micro (Seed {seed})')\n    axs[0].set_xlabel(\"Epochs\")\n    axs[0].set_ylabel(\"Loss\")\n    axs[0].legend()\n\n    axs[1].plot(train_f1s, label=f'Training Micro F1', color=\"cyan\")\n    axs[1].plot(val_f1s, label=f'Validation Micro F1', linestyle='--', color=\"magenta\")\n    #axs[1].set_title(f'{model_name} - Train and Val F1 micro Score (Seed {seed})')\n    axs[1].set_xlabel(\"Epochs\")\n    axs[1].set_ylabel(\"Micro F1\")\n    axs[1].legend()\n\n    plt.tight_layout()\n    plt.savefig(f\"/kaggle/working/train_val_loss_f1_{model_name}_{seed}.png\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_CONFIGS = {\n    \"PubMedBERT\": {\n        \"model_name\": \"NeuML/pubmedbert-base-embeddings\",\n        \"tokenizer\": \"NeuML/pubmedbert-base-embeddings\"\n    },\n    \n    \"BERT\": {\n        \"model_name\": \"bert-base-uncased\",\n        \"tokenizer\": \"bert-base-uncased\"\n    },\n    \"BioBERT\": {\n        \"model_name\": \"dmis-lab/biobert-v1.1\",\n        \"tokenizer\": \"dmis-lab/biobert-v1.1\"\n    }\n}\n\nMODEL_CONFIGS = {\n    \"PubMedBERT\": {\n        \"model_name\": \"NeuML/pubmedbert-base-embeddings\",\n        \"tokenizer\": \"NeuML/pubmedbert-base-embeddings\"\n    }\n    }\n\nspecial_tokens = ['<ent1>', '</ent1>', '<ent2>', '</ent2>']\n#NUM_SEEDS = 5\n#seeds = [42] + random.sample(range(1, 1000), 4) \nseeds = [42]\nprint (seeds)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nMODEL_SAVE_DIR = \"/kaggle/working/best_models/\"\nos.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n\nbest_models = {}\nresults = []\n\nfor model_name in MODEL_CONFIGS.keys():\n    BATCH_SIZE = 16\n    NUM_EPOCHS = 6\n    LR = 1e-5\n    WEIGHT_DECAY = 0.01 \n    DROPOUT = 0.1 \n    MAX_NORM = 1.0\n    THRESHOLD = 0.6 # treshold for predicting positive class\n\n    # add special tokens to tokenizer (entity markers)\n    tokenizer_str = MODEL_CONFIGS[model_name][\"tokenizer\"]\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_str)\n    tokenizer.add_tokens(special_tokens)\n    tokenizer_voc_size = len(tokenizer)\n    vocab = tokenizer.get_vocab()\n    last_four_token_ids = sorted(vocab.values())[-4:] # extract the last four token ids (=entity markers)\n    \n    ent1_start_id, ent1_end_id, ent2_start_id, ent2_end_id = last_four_token_ids \n\n    train_dataloader, val_dataloader, test_dataloader = create_dataloaders(BATCH_SIZE,tokenizer, DEVICE) # create the data loaders\n\n    test_micro_f1_scores = []\n    test_macro_f1_scores = []\n    all_train_losses = []\n    all_val_losses = []\n    all_train_f1s_micro = []\n    all_train_f1s_macro = []\n    all_val_f1s_micro = []\n    all_val_f1s_macro = []\n    \n    best_micro_f1 = -1\n    best_macro_f1 = -1\n    best_model_state = None\n    best_model_seed = None\n\n    for seed in seeds:\n        wandb.init(\n        project=\"Relation_Classification\",\n        entity=\"lp2\",\n        config={\n        \"model\": model_name,\n        \"learning_rate\": LR,\n        \"batch_size\": BATCH_SIZE,\n        \"epochs\": NUM_EPOCHS,\n        \"dropout\": DROPOUT,\n        \"weight_decay\": WEIGHT_DECAY,\n        \"max_norm\": MAX_NORM\n        },\n        name=f\"{model_name}_seed{seed}_{THRESHOLD}\",\n        group=model_name,  # Groups all runs for a given model together\n        tags=[model_name, f\"seed-{seed}\"])\n        config = wandb.config\n\n        # train and evaluate model\n        model, test_micro_f1, test_macro_f1, train_losses, val_losses, train_f1s_micro, train_f1_macro, val_f1s_micro, val_f1_macro = train_and_evaluate(\n            model_name, tokenizer_voc_size, seed, train_dataloader, val_dataloader, test_dataloader, LR, WEIGHT_DECAY, NUM_EPOCHS, DROPOUT, DEVICE, MAX_NORM, ent1_start_id, ent1_end_id, ent2_start_id, ent2_end_id, THRESHOLD) \n        \n        test_micro_f1_scores.append(test_micro_f1)\n        test_macro_f1_scores.append(test_macro_f1)\n        all_train_losses.append(train_losses)\n        all_val_losses.append(val_losses)\n        all_train_f1s_micro.append(train_f1s_micro)\n        all_train_f1s_macro.append(train_f1_macro) \n        all_val_f1s_micro.append(val_f1s_micro) \n        all_val_f1s_macro.append(val_f1_macro)\n        \n        plot_train_val_metrics(train_losses, val_losses, train_f1s_micro, val_f1s_micro, model_name, seed)\n\n        # save the best model \n        if test_micro_f1 > best_micro_f1:\n            best_micro_f1 = test_micro_f1\n            best_model_state = model.state_dict()\n            best_model_seed = seed\n\n        seed_model_path = os.path.join(MODEL_SAVE_DIR, f\"{model_name}_{seed}.pt\")\n        torch.save(model.state_dict(), seed_model_path)\n\n    best_model_path = os.path.join(MODEL_SAVE_DIR, f\"{model_name}_best.pt\")\n    best_models[model_name] = best_model_path\n    torch.save(best_model_state, best_model_path)\n\n    # get mean and std of micro and macro F1s\n    mean_micro_f1 = np.mean(test_micro_f1_scores)\n    std_micro_f1 = np.std(test_micro_f1_scores)\n    mean_macro_f1 = np.mean(test_macro_f1_scores)\n    std_macro_f1 = np.std(test_macro_f1_scores)\n    \n    results.append({\n        \"model\": model_name,\n        \"avg_test_micro_f1\": mean_micro_f1,\n        \"std_test_micro_f1\": std_micro_f1,\n        \"avg_test_macro_f1\": mean_macro_f1,\n        \"std_test_macro_f1\": std_macro_f1,\n        \"best_micro_f1\": best_micro_f1,\n        \"best_macro_f1\": best_macro_f1,\n        \"best_model_seed\": best_model_seed,\n        \"best_model_path\": best_model_path\n    })\n\n\nresults_df = pd.DataFrame(results)\nresults_json_path = \"/kaggle/working/relation_classification_results.json\"\nresults_df.to_json(results_json_path, orient=\"records\", indent=4)\n\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:45:26.288192Z","iopub.execute_input":"2025-04-04T06:45:26.288508Z","iopub.status.idle":"2025-04-04T06:46:00.628895Z","shell.execute_reply.started":"2025-04-04T06:45:26.288481Z","shell.execute_reply":"2025-04-04T06:46:00.627977Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation: Step 1: Generating predictions\n\nPrepare evaluation in the form that is required for the official evaluation script of GutBrainIE2025. \n\nGiven a title+abstract (one article ID) of the test set:\n\n1) generate candidates. This can be done by enumerating all combinations of entity pairs in the ground truth OR using my own NER model to extract entitites + their spans + labels.\n2) predict whether there is a relationship between them. Store the labels (as tuple) in a list of relations for this article ID.\n3) generate a set with ordered tuples and create the output format.","metadata":{}},{"cell_type":"code","source":"# replace model with best model and corresponding tokenizer\nmodel = model.eval()\ntokenizer = tokenizer \nTHRESHOLD_INFERENCE = 0.85\nTHRESHOLD = 0.6\nuse_ground_truth = True\n\n\"\"\"else:\n        entities = run_ner_model(text) \"\"\"\npredictions = {}\n\n# ground truth path for NERs \nGROUND_TRUTH_PATH = \"/kaggle/input/gutbrainie2025/gutbrainie2025/Annotations/Dev/json_format/dev.json\"\n\nwith open(GROUND_TRUTH_PATH, \"r\", encoding=\"utf-8\") as f:\n    ground_truth_data = json.load(f)\n    \ndef get_ground_truth_entities(abstract_id):\n    # get entities for that abstract id\n    article_data = ground_truth_data.get(abstract_id, {})\n    entities = article_data.get(\"entities\", [])\n    return entities\n\ndef generate_candidate_pairs(entities):\n    candidate_pairs = [] # get all possible candidate pairs\n    for i in range(len(entities)):\n        for j in range(len(entities)):\n            if i == j: # CHECK WHETHER THIS COULD TECHNICALLY BE POSSIBLE THAT SUBJ = OBJ, reflexive relationships?\n                continue\n            candidate_pairs.append((entities[i], entities[j])) # entities look like this: {'start_idx': 0, 'end_idx': 26, 'location': 'title', 'text_span': 'Lactobacillus fermentum NS9', 'label': 'dietary supplement'}\n    return candidate_pairs\n    \n    \nif use_ground_truth:\n    for abstract_id, article_data in tqdm(ground_truth_data.items(), desc=\"Processing Abstracts\", unit=\"abstract\"):\n        entities = get_ground_truth_entities(abstract_id)\n        candidate_pairs = generate_candidate_pairs(entities)\n\n        metadata = article_data.get(\"metadata\", {})\n        title = metadata.get(\"title\", \"\")\n        abstract = metadata.get(\"abstract\", \"\")\n        full_text = (title + \" \" + abstract).strip()  # get the combination of text and abstract\n        offset = len(title) + 1 # offset for abstract positions\n        \n        for entity1, entity2 in candidate_pairs:\n            subj_start_idx, subj_end_idx = get_adjusted_indices(entity1, offset)\n            obj_start_idx, obj_end_idx  = get_adjusted_indices(entity2, offset)\n            \n            # insert entity markers in the text for entity1 and entity2\n            marked_text = mark_entities(full_text, subj_start_idx, subj_end_idx, obj_start_idx, obj_end_idx)\n        \n            inputs = tokenizer(marked_text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n            # run through the relation classifier\n            with torch.no_grad():\n                output = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n            relation_exists = (torch.sigmoid(output) > THRESHOLD_INFERENCE).item() # returns 1 if relation, 0 else\n        \n            if relation_exists:\n                rel_info = {\"subject_label\": entity1[\"label\"], \"object_label\": entity2[\"label\"]}\n                if abstract_id not in predictions:\n                    predictions[abstract_id] = {\"binary_tag_based_relations\": []}\n                if rel_info not in predictions[abstract_id][\"binary_tag_based_relations\"]:\n                    predictions[abstract_id][\"binary_tag_based_relations\"].append(rel_info)\n\nwith open(f\"relation_predictions_{model_name}.json\", \"w\") as f:\n    json.dump(predictions, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:46:08.056845Z","iopub.execute_input":"2025-04-04T06:46:08.057243Z","iopub.status.idle":"2025-04-04T07:00:32.901080Z","shell.execute_reply.started":"2025-04-04T06:46:08.057209Z","shell.execute_reply":"2025-04-04T07:00:32.900270Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 2: using challenge script to calculate metrics (cf. https://github.com/MMartinelli-hub/GutBrainIE_2025_Baseline/blob/main/Eval/evaluate.py)","metadata":{}},{"cell_type":"code","source":"# DEFINE HERE THE PATH(S) TO YOUR PREDICTIONS\n#PREDICTIONS_PATH_6_1 = 'org_T61_BaselineRun_NuNerZero.json'\nPREDICTIONS_PATH_6_2 = \"/kaggle/working/relation_predictions_PubMedBERT.json\"\n#PREDICTIONS_PATH_6_3 = 'org_T622_BaselineRun_ATLOP.json'\n#PREDICTIONS_PATH_6_4 = 'org_T623_BaselineRun_ATLOP.json'\n\n# DEFINE HERE FOR WHICH SUBTASK(S) YOU WANT TO EVAL YOUR PREDICTIONS\neval_6_1_NER = False\neval_6_2_binary_tag_RE = True\neval_6_3_ternary_tag_RE = False\neval_6_4_ternary_mention_RE = False\n\nGROUND_TRUTH_PATH = \"/kaggle/input/gutbrainie2025/gutbrainie2025/Annotations/Dev/json_format/dev.json\"\ntry:\n    with open(GROUND_TRUTH_PATH, 'r', encoding='utf-8') as file:\n        ground_truth = json.load(file)\nexcept OSError:\n    raise OSError(f'Error in opening the specified json file: {GROUND_TRUTH_PATH}')\n\nLEGAL_ENTITY_LABELS = [\n    \"anatomical location\",\n    \"animal\",\n    \"bacteria\",\n    \"biomedical technique\",\n    \"chemical\",\n    \"DDF\",\n    \"dietary supplement\",\n    \"drug\",\n    \"food\",\n    \"gene\",\n    \"human\",\n    \"microbiome\",\n    \"statistical technique\"\n]\n\nLEGAL_RELATION_LABELS = [\n    \"administered\",\n    \"affect\",\n    \"change abundance\",\n    \"change effect\",\n    \"change expression\",\n    \"compared to\",\n    \"impact\",\n    \"influence\",\n    \"interact\",\n    \"is a\",\n    \"is linked to\",\n    \"located in\",\n    \"part of\",\n    \"produced by\",\n    \"strike\",\n    \"target\",\n    \"used by\"\n]\n\n\ndef eval_submission_6_1_NER(path):\n    try:\n        with open(path, 'r', encoding='utf-8') as file:\n            predictions = json.load(file)\n    except OSError:\n        raise OSError(f'Error in opening the specified json file: {path}')\n    \n    ground_truth_NER = dict()\n    count_annotated_entities_per_label = {}\n    \n    for pmid, article in ground_truth.items():\n        if pmid not in ground_truth_NER:\n            ground_truth_NER[pmid] = []\n        for entity in article['entities']:\n            start_idx = int(entity[\"start_idx\"])\n            end_idx = int(entity[\"end_idx\"])\n            location = str(entity[\"location\"])\n            text_span = str(entity[\"text_span\"])\n            label = str(entity[\"label\"]) \n            \n            entry = (start_idx, end_idx, location, text_span, label)\n            ground_truth_NER[pmid].append(entry)\n            \n            if label not in count_annotated_entities_per_label:\n                count_annotated_entities_per_label[label] = 0\n            count_annotated_entities_per_label[label] += 1\n\n    count_predicted_entities_per_label = {label: 0 for label in list(count_annotated_entities_per_label.keys())}\n    count_true_positives_per_label = {label: 0 for label in list(count_annotated_entities_per_label.keys())}\n\n    for pmid in predictions.keys():\n        try:\n            entities = predictions[pmid]['entities']\n        except KeyError:\n            raise KeyError(f'{pmid} - Not able to find field \\\"entities\\\" within article')\n        \n        for entity in entities:\n            try:\n                start_idx = int(entity[\"start_idx\"])\n                end_idx = int(entity[\"end_idx\"])\n                location = str(entity[\"location\"])\n                text_span = str(entity[\"text_span\"])\n                label = str(entity[\"label\"]) \n            except KeyError:\n                raise KeyError(f'{pmid} - Not able to find one or more of the expected fields for entity: {entity}')\n            \n            if label not in LEGAL_ENTITY_LABELS:\n                raise NameError(f'{pmid} - Illegal label {label} for entity: {entity}')\n\n            if label in count_predicted_entities_per_label:\n                count_predicted_entities_per_label[label] += 1\n\n            entry = (start_idx, end_idx, location, text_span, label)\n            if entry in ground_truth_NER[pmid]:\n                count_true_positives_per_label[label] += 1\n\n    count_annotated_entities = sum(count_annotated_entities_per_label[label] for label in list(count_annotated_entities_per_label.keys()))\n    count_predicted_entities = sum(count_predicted_entities_per_label[label] for label in list(count_annotated_entities_per_label.keys()))\n    count_true_positives = sum(count_true_positives_per_label[label] for label in list(count_annotated_entities_per_label.keys()))\n\n    micro_precision = count_true_positives / (count_predicted_entities + 1e-10)\n    micro_recall = count_true_positives / (count_annotated_entities + 1e-10)\n    micro_f1 = 2 * ((micro_precision * micro_recall) / (micro_precision + micro_recall + 1e-10))\n\n    precision, recall, f1 = 0, 0, 0\n    n = 0\n    for label in list(count_annotated_entities_per_label.keys()):\n        n += 1\n        current_precision = count_true_positives_per_label[label] / (count_predicted_entities_per_label[label] + 1e-10) \n        current_recall = count_true_positives_per_label[label] / (count_annotated_entities_per_label[label] + 1e-10) \n        \n        precision += current_precision\n        recall += current_recall\n        f1 += 2 * ((current_precision * current_recall) / (current_precision + current_recall + 1e-10))\n    \n    precision = precision / n\n    recall = recall / n\n    f1 = f1 / n\n\n    return precision, recall, f1, micro_precision, micro_recall, micro_f1\n\ndef eval_submission_6_2_binary_tag_RE(path):\n    try:\n        with open(path, 'r', encoding='utf-8') as file:\n            predictions = json.load(file)\n    except OSError:\n        raise OSError(f'Error in opening the specified json file: {path}')\n    \n    ground_truth_binary_tag_RE = dict()\n    count_annotated_relations_per_label = {}\n\n    for pmid, article in ground_truth.items():\n        if pmid not in ground_truth_binary_tag_RE:\n            ground_truth_binary_tag_RE[pmid] = []\n        for relation in article['binary_tag_based_relations']:\n            subject_label = str(relation[\"subject_label\"])\n            object_label = str(relation[\"object_label\"]) \n\n            label = (subject_label, object_label)\n            ground_truth_binary_tag_RE[pmid].append(label)\n\n            if label not in count_annotated_relations_per_label:\n                count_annotated_relations_per_label[label] = 0\n            count_annotated_relations_per_label[label] += 1\n    \n    count_predicted_relations_per_label = {label: 0 for label in list(count_annotated_relations_per_label.keys())}\n    count_true_positives_per_label = {label: 0 for label in list(count_annotated_relations_per_label.keys())}\n\n    for pmid in predictions.keys():\n        try:\n            relations = predictions[pmid]['binary_tag_based_relations']\n        except KeyError:\n            raise KeyError(f'{pmid} - Not able to find field \\\"binary_tag_based_relations\\\" within article')\n        \n        for relation in relations:\n            try:\n                subject_label = str(relation[\"subject_label\"])\n                object_label = str(relation[\"object_label\"]) \n            except KeyError:\n                raise KeyError(f'{pmid} - Not able to find one or more of the expected fields for relation: {relation}')\n            \n            if subject_label not in LEGAL_ENTITY_LABELS:\n                raise NameError(f'{pmid} - Illegal subject entity label {subject_label} for relation: {relation}')\n            \n            if object_label not in LEGAL_ENTITY_LABELS:\n                raise NameError(f'{pmid} - Illegal object entity label {object_label} for relation: {relation}')\n\n            label = (subject_label, object_label)\n            if label in count_predicted_relations_per_label:\n                count_predicted_relations_per_label[label] += 1\n\n            if label in ground_truth_binary_tag_RE[pmid]:\n                count_true_positives_per_label[label] += 1\n\n    count_annotated_relations = sum(count_annotated_relations_per_label[label] for label in list(count_annotated_relations_per_label.keys()))\n    count_predicted_relations = sum(count_predicted_relations_per_label[label] for label in list(count_annotated_relations_per_label.keys()))\n    count_true_positives = sum(count_true_positives_per_label[label] for label in list(count_annotated_relations_per_label.keys()))\n\n    micro_precision = count_true_positives / (count_predicted_relations + 1e-10)\n    micro_recall = count_true_positives / (count_annotated_relations + 1e-10)\n    micro_f1 = 2 * ((micro_precision * micro_recall) / (micro_precision + micro_recall + 1e-10))\n\n    precision, recall, f1 = 0, 0, 0\n    n = 0\n    for label in list(count_annotated_relations_per_label.keys()):\n        n += 1\n        current_precision = count_true_positives_per_label[label] / (count_predicted_relations_per_label[label] + 1e-10) \n        current_recall = count_true_positives_per_label[label] / (count_annotated_relations_per_label[label] + 1e-10) \n        \n        precision += current_precision\n        recall += current_recall\n        f1 += 2 * ((current_precision * current_recall) / (current_precision + current_recall + 1e-10))\n    \n    precision = precision / n\n    recall = recall / n\n    f1 = f1 / n\n\n    return precision, recall, f1, micro_precision, micro_recall, micro_f1\n\n\ndef eval_submission_6_3_ternary_tag_RE(path):\n    try:\n        with open(path, 'r', encoding='utf-8') as file:\n            predictions = json.load(file)\n    except OSError:\n        raise OSError(f'Error in opening the specified json file: {path}')\n    \n    ground_truth_ternary_tag_RE = dict()\n    count_annotated_relations_per_label = {}\n\n    for pmid, article in ground_truth.items():\n        if pmid not in ground_truth_ternary_tag_RE:\n            ground_truth_ternary_tag_RE[pmid] = []\n        for relation in article['ternary_tag_based_relations']:\n            subject_label = str(relation[\"subject_label\"])\n            predicate = str(relation[\"predicate\"])\n            object_label = str(relation[\"object_label\"]) \n            \n            label = (subject_label, predicate, object_label)\n            ground_truth_ternary_tag_RE[pmid].append(label)\n\n            if label not in count_annotated_relations_per_label:\n                count_annotated_relations_per_label[label] = 0\n            count_annotated_relations_per_label[label] += 1\n\n    count_predicted_relations_per_label = {label: 0 for label in list(count_annotated_relations_per_label.keys())}\n    count_true_positives_per_label = {label: 0 for label in list(count_annotated_relations_per_label.keys())}\n\n    for pmid in predictions.keys():\n        try:\n            relations = predictions[pmid]['ternary_tag_based_relations']\n        except KeyError:\n            raise KeyError(f'{pmid} - Not able to find field \\\"ternary_tag_based_relations\\\" within article')\n        \n        for relation in relations:            \n            try:\n                subject_label = str(relation[\"subject_label\"])\n                predicate = str(relation[\"predicate\"])\n                object_label = str(relation[\"object_label\"]) \n            except KeyError:\n                raise KeyError(f'{pmid} - Not able to find one or more of the expected fields for relation: {relation}')\n            \n            if subject_label not in LEGAL_ENTITY_LABELS:\n                raise NameError(f'{pmid} - Illegal subject entity label {subject_label} for relation: {relation}')\n            \n            if object_label not in LEGAL_ENTITY_LABELS:\n                raise NameError(f'{pmid} - Illegal object entity label {object_label} for relation: {relation}')\n            \n            if predicate not in LEGAL_RELATION_LABELS:\n                raise NameError(f'{pmid} - Illegal predicate {predicate} for relation: {relation}')\n\n            label = (subject_label, predicate, object_label)\n            if label in count_predicted_relations_per_label:\n                count_predicted_relations_per_label[label] += 1\n\n            if label in ground_truth_ternary_tag_RE[pmid]:\n                count_true_positives_per_label[label] += 1\n\n    count_annotated_relations = sum(count_annotated_relations_per_label[label] for label in list(count_annotated_relations_per_label.keys()))\n    count_predicted_relations = sum(count_predicted_relations_per_label[label] for label in list(count_annotated_relations_per_label.keys()))\n    count_true_positives = sum(count_true_positives_per_label[label] for label in list(count_annotated_relations_per_label.keys()))\n\n    micro_precision = count_true_positives / (count_predicted_relations + 1e-10)\n    micro_recall = count_true_positives / (count_annotated_relations + 1e-10)\n    micro_f1 = 2 * ((micro_precision * micro_recall) / (micro_precision + micro_recall + 1e-10))\n\n    precision, recall, f1 = 0, 0, 0\n    n = 0\n    for label in list(count_annotated_relations_per_label.keys()):\n        n += 1\n        current_precision = count_true_positives_per_label[label] / (count_predicted_relations_per_label[label] + 1e-10) \n        current_recall = count_true_positives_per_label[label] / (count_annotated_relations_per_label[label] + 1e-10) \n        \n        precision += current_precision\n        recall += current_recall\n        f1 += 2 * ((current_precision * current_recall) / (current_precision + current_recall + 1e-10))\n    \n    precision = precision / n\n    recall = recall / n\n    f1 = f1 / n\n\n    return precision, recall, f1, micro_precision, micro_recall, micro_f1\n\n\ndef eval_submission_6_4_ternary_mention_RE(path):\n    try:\n        with open(path, 'r', encoding='utf-8') as file:\n            predictions = json.load(file)\n    except OSError:\n        raise OSError(f'Error in opening the specified json file: {path}')\n    \n    ground_truth_ternary_mention_RE = dict()\n    count_annotated_relations_per_label = {}\n\n    for pmid, article in ground_truth.items():\n        if pmid not in ground_truth_ternary_mention_RE:\n            ground_truth_ternary_mention_RE[pmid] = []\n        for relation in article['ternary_mention_based_relations']:\n            subject_text_span = str(relation[\"subject_text_span\"])\n            subject_label = str(relation[\"subject_label\"])\n            predicate = str(relation[\"predicate\"])\n            object_text_span = str(relation[\"object_text_span\"])\n            object_label = str(relation[\"object_label\"]) \n\n            entry = (subject_text_span, subject_label, predicate, object_text_span, object_label)\n            ground_truth_ternary_mention_RE[pmid].append(entry)\n\n            label = (subject_label, predicate, object_label)\n            if label not in count_annotated_relations_per_label:\n                count_annotated_relations_per_label[label] = 0\n            count_annotated_relations_per_label[label] += 1\n\n    count_predicted_relations_per_label = {label: 0 for label in list(count_annotated_relations_per_label.keys())}\n    count_true_positives_per_label = {label: 0 for label in list(count_annotated_relations_per_label.keys())}\n    \n    for pmid in predictions.keys():\n        try:\n            relations = predictions[pmid]['ternary_mention_based_relations']\n        except KeyError:\n            raise KeyError(f'{pmid} - Not able to find field \\\"ternary_mention_based_relations\\\" within article')\n        \n        for relation in relations:\n            try:\n                subject_text_span = str(relation[\"subject_text_span\"])\n                subject_label = str(relation[\"subject_label\"])\n                predicate = str(relation[\"predicate\"])\n                object_text_span = str(relation[\"object_text_span\"])\n                object_label = str(relation[\"object_label\"]) \n            except KeyError:\n                raise KeyError(f'{pmid} - Not able to find one or more of the expected fields for relation: {relation}')\n            \n            if subject_label not in LEGAL_ENTITY_LABELS:\n                raise NameError(f'{pmid} - Illegal subject entity label {subject_label} for relation: {relation}')\n            \n            if object_label not in LEGAL_ENTITY_LABELS:\n                raise NameError(f'{pmid} - Illegal object entity label {object_label} for relation: {relation}')\n            \n            if predicate not in LEGAL_RELATION_LABELS:\n                raise NameError(f'{pmid} - Illegal predicate {predicate} for relation: {relation}')\n                        \n            entry = (subject_text_span, subject_label, predicate, object_text_span, object_label)\n            label = (subject_label, predicate, object_label) \n            \n            if label in count_predicted_relations_per_label:\n                count_predicted_relations_per_label[label] += 1\n            \n            if entry in ground_truth_ternary_mention_RE[pmid]:\n                count_true_positives_per_label[label] += 1\n    \n    count_annotated_relations = sum(count_annotated_relations_per_label[label] for label in list(count_annotated_relations_per_label.keys()))\n    count_predicted_relations = sum(count_predicted_relations_per_label[label] for label in list(count_annotated_relations_per_label.keys()))\n    count_true_positives = sum(count_true_positives_per_label[label] for label in list(count_annotated_relations_per_label.keys()))\n\n    micro_precision = count_true_positives / (count_predicted_relations + 1e-10)\n    micro_recall = count_true_positives / (count_annotated_relations + 1e-10)\n    micro_f1 = 2 * ((micro_precision * micro_recall) / (micro_precision + micro_recall + 1e-10))\n\n    precision, recall, f1 = 0, 0, 0\n    n = 0\n    for label in list(count_annotated_relations_per_label.keys()):\n        n += 1\n        current_precision = count_true_positives_per_label[label] / (count_predicted_relations_per_label[label] + 1e-10) \n        current_recall = count_true_positives_per_label[label] / (count_annotated_relations_per_label[label] + 1e-10) \n        \n        precision += current_precision\n        recall += current_recall\n        f1 += 2 * ((current_precision * current_recall) / (current_precision + current_recall + 1e-10))\n    \n    precision = precision / n\n    recall = recall / n\n    f1 = f1 / n\n\n    return precision, recall, f1, micro_precision, micro_recall, micro_f1\n\n\nif __name__ == '__main__':\n    round_to_decimal_position = 4\n\n    if eval_6_1_NER:\n        precision, recall, f1, micro_precision, micro_recall, micro_f1 = eval_submission_6_1_NER(PREDICTIONS_PATH_6_1)\n        print(\"\\n\\n=== 6_1_NER ===\")\n        print(f\"Macro-precision: {round(precision, round_to_decimal_position)}\")\n        print(f\"Macro-recall: {round(recall, round_to_decimal_position)}\")\n        print(f\"Macro-F1: {round(f1, round_to_decimal_position)}\")\n        print(f\"Micro-precision: {round(micro_precision, round_to_decimal_position)}\")\n        print(f\"Micro-recall: {round(micro_recall, round_to_decimal_position)}\")\n        print(f\"Micro-F1: {round(micro_f1, round_to_decimal_position)}\")\n\n    \"\"\"if eval_6_2_binary_tag_RE:\n        precision, recall, f1, micro_precision, micro_recall, micro_f1 = eval_submission_6_2_binary_tag_RE(PREDICTIONS_PATH_6_2)\n        print(\"\\n\\n=== 6_2_binary_tag_RE ===\")\n        print(f\"Macro-precision: {round(precision, round_to_decimal_position)}\")\n        print(f\"Macro-recall: {round(recall, round_to_decimal_position)}\")\n        print(f\"Macro-F1: {round(f1, round_to_decimal_position)}\")\n        print(f\"Micro-precision: {round(micro_precision, round_to_decimal_position)}\")\n        print(f\"Micro-recall: {round(micro_recall, round_to_decimal_position)}\")\n        print(f\"Micro-F1: {round(micro_f1, round_to_decimal_position)}\")\"\"\"\n\n    if eval_6_2_binary_tag_RE:\n        precision, recall, f1, micro_precision, micro_recall, micro_f1 = eval_submission_6_2_binary_tag_RE(PREDICTIONS_PATH_6_2)\n    \n        round_to = round_to_decimal_position  # Just for readability\n        results_text = (\n        \"\\n\\n=== 6_2_binary_tag_RE ===\\n\"\n        f\"Macro-precision: {round(precision, round_to)}\\n\"\n        f\"Macro-recall: {round(recall, round_to)}\\n\"\n        f\"Macro-F1: {round(f1, round_to)}\\n\"\n        f\"Micro-precision: {round(micro_precision, round_to)}\\n\"\n        f\"Micro-recall: {round(micro_recall, round_to)}\\n\"\n        f\"Micro-F1: {round(micro_f1, round_to)}\\n\"\n        )\n    \n    print(results_text)  # Print to console\n\n    # Save results to a text file\n    with open(\"evaluation_results.txt\", \"w\", encoding=\"utf-8\") as file:\n        file.write(results_text)\n\n    if eval_6_3_ternary_tag_RE:\n        precision, recall, f1, micro_precision, micro_recall, micro_f1 = eval_submission_6_3_ternary_tag_RE(PREDICTIONS_PATH_6_3)\n        print(\"\\n\\n=== 6_3_ternary_tag_RE ===\")\n        print(f\"Macro-precision: {round(precision, round_to_decimal_position)}\")\n        print(f\"Macro-recall: {round(recall, round_to_decimal_position)}\")\n        print(f\"Macro-F1: {round(f1, round_to_decimal_position)}\")\n        print(f\"Micro-precision: {round(micro_precision, round_to_decimal_position)}\")\n        print(f\"Micro-recall: {round(micro_recall, round_to_decimal_position)}\")\n        print(f\"Micro-F1: {round(micro_f1, round_to_decimal_position)}\")\n\n    if eval_6_4_ternary_mention_RE:\n        precision, recall, f1, micro_precision, micro_recall, micro_f1 = eval_submission_6_4_ternary_mention_RE(PREDICTIONS_PATH_6_4)\n        print(\"\\n\\n=== 6_4_ternary_mention_RE ===\")\n        print(f\"Macro-precision: {round(precision, round_to_decimal_position)}\")\n        print(f\"Macro-recall: {round(recall, round_to_decimal_position)}\")\n        print(f\"Macro-F1: {round(f1, round_to_decimal_position)}\")\n        print(f\"Micro-precision: {round(micro_precision, round_to_decimal_position)}\")\n        print(f\"Micro-recall: {round(micro_recall, round_to_decimal_position)}\")\n        print(f\"Micro-F1: {round(micro_f1, round_to_decimal_position)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:01:45.536818Z","iopub.execute_input":"2025-04-04T07:01:45.537258Z","iopub.status.idle":"2025-04-04T07:01:45.602269Z","shell.execute_reply.started":"2025-04-04T07:01:45.537231Z","shell.execute_reply":"2025-04-04T07:01:45.601567Z"}},"outputs":[],"execution_count":null}]}