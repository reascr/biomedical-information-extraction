{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11716507,"sourceType":"datasetVersion","datasetId":7354617},{"sourceId":11720108,"sourceType":"datasetVersion","datasetId":7357279}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Investigating the optimal inference threshold for binary RE","metadata":{}},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import precision_recall_curve\nimport numpy as np\nimport pandas as pd\n\nwith open('/kaggle/input/predictions/Predictions_mention_based_PubMedBERT_29.json') as f: # predictions\n    preds = json.load(f)\nwith open('/kaggle/input/predictions/dev.json') as f: # ground truth\n    gts = json.load(f)\n\n\ntrue_pairs = set() # get true relations from the ground truth\nfor article_id, data in gts.items():\n    for rel in data.get('relations', []):\n        key = (rel['subject_start_idx'], rel['object_start_idx'],rel['subject_label'],rel['object_label']) # just take start idx  because we do not consider span based errors in binary RE (heuristic)\n        true_pairs.add(key)\n\nscores = []\nlabels = []  # correct (TP), incorrect (FP)\n\nfor article_id, doc in preds.items():\n    for rel in doc.get('binary_mention_based_relations', []):\n        key = (rel['subject_start_index'], rel['object_start_index'],rel['subject_label'],rel['object_label'])\n        scores.append(rel['score'])\n        labels.append(1 if key in true_pairs else 0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame({\n    'Confidence score': scores,\n    'Correctness': ['TP' if l else 'FP' for l in labels]\n})\n\npastel_green = '#a8e6a8'\npastel_red = '#e6b8b8' \n\nplt.figure(figsize=(6, 5))\nsns.boxplot(x='Correctness', y='Confidence score', data=df, palette={'TP': pastel_green, 'FP': pastel_red})\n\nplt.tight_layout()\nplt.savefig(\"binary_RE_box_Plot\")\nplt.show()\ndf.groupby('Correctness')['Confidence score'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tp_scores = [s for s, l in zip(scores, labels) if l == 1]\nfp_scores = [s for s, l in zip(scores, labels) if l == 0]\nplt.figure(figsize=(8, 5))\n\n# make a histrogram of FP and TP distributions\nplt.hist(fp_scores, bins=20, alpha=0.6, label='False Positives', color=pastel_red, edgecolor='grey')\nplt.hist(tp_scores, bins=20, alpha=0.7, label='True Positives', color=pastel_green, edgecolor='grey')\nplt.xlabel('Confidence Score')\nplt.ylabel('Frequency')\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# zoomed in histogram for high confidence predictions\nbins = np.arange(0.9, 1.005, 0.005)\n\nplt.figure(figsize=(8, 5))\nplt.hist(fp_scores, bins=bins, alpha=0.6, label='FPs', color=pastel_red, edgecolor='grey')\nplt.hist(tp_scores, bins=bins, alpha=0.7, label='TPs', color=pastel_green, edgecolor='grey')\nxticks = np.arange(0.90, 1.01, 0.01)\nplt.xticks(xticks, [f\"{x:.2f}\" for x in xticks])\n\nplt.xlabel('Confidence Score')\nplt.ylabel('Frequency')\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"histogram_tp_fp.png\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# \"Compute precision-recall pairs for different probability thresholds\", cf. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html, assessed 7th May 2025\n# This is for mention based binary RE, it is just an approximation for tag based RE... , cf. https://www.blog.trainindata.com/precision-recall-curves/, assessed 7th May 2025\n\nscores = np.array(scores) \nlabels = np.array(labels) # true labels\n\nprecision, recall, thresholds = precision_recall_curve(labels, scores) # get precision and recall \nf1_scores = 2 * (precision * recall) / (precision + recall + 1e-12) # compute f1s for every threshold, add small value to avoid zero division error\n\nbest_idx       = np.argmax(f1_scores)\nbest_thresh    = thresholds[best_idx]\nbest_precision = precision[best_idx]\nbest_recall    = recall[best_idx]\nbest_f1        = f1_scores[best_idx]\n\nprint(f\"Best threshold = {best_thresh}\")\nprint(f\"Precision = {best_precision}\")\nprint(f\"Recall = {best_recall}\")\nprint(f\"F1 = {best_f1}\")\n\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label='Precision-Recall curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend(loc='best')\n# Plot the threshold points\nplt.scatter(recall[best_idx], precision[best_idx], color='red', label=f'Best Threshold = {best_thresh} (F1 = {best_f1})')\nplt.legend(loc='best')\nplt.savefig(\"precision-recall_curve.png\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve, average_precision_score\nimport matplotlib.pyplot as plt\nwith open('/kaggle/input/predictions/Predictions_mention_based_PubMedBERT_29.json') as f: # predictions\n    preds = json.load(f)\nwith open('/kaggle/input/predictions/dev.json') as f: # ground truth\n    gts = json.load(f)\n\n\ntrue_pairs = set() # get true relations from the ground truth\nfor article_id, data in gts.items():\n    for rel in data.get('relations', []):\n        key = (rel['subject_start_idx'], rel['object_start_idx'],rel['subject_label'],rel['object_label']) # just take start idx  because we do not consider span based errors in binary RE \n        true_pairs.add(key)\n\nscores = []\nlabels = []  # correct (TP), incorrect (FP)\n\nfor article_id, doc in preds.items():\n    for rel in doc.get('binary_mention_based_relations', []):\n        key = (rel['subject_start_index'], rel['object_start_index'],rel['subject_label'],rel['object_label'])\n        scores.append(rel['score'])\n        labels.append(1 if key in true_pairs else 0)\n\npred_pairs = {\n    (rel['subject_start_index'], rel['object_start_index'],rel['subject_label'],rel['object_label'])\n    for doc in preds.values()\n    for rel in doc.get('binary_mention_based_relations', [])\n}\n\n# include FNs, i.e. missing ground truth positives\nfor true_pair in true_pairs:\n    if true_pair not in pred_pairs:\n        scores.append(0.0) # assign 0 score... This is only a heuristic since the predictions do only include positive relations.\n        labels.append(1)\n\nprecision, recall, thresholds = precision_recall_curve(labels, scores)\nf1_scores = 2 * (precision * recall) / (precision + recall + 1e-12) # compute f1s for every threshold, add small value to avoid zero division error\n\nbest_idx       = np.argmax(f1_scores)\nbest_thresh    = thresholds[best_idx]\nbest_precision = precision[best_idx]\nbest_recall    = recall[best_idx]\nbest_f1        = f1_scores[best_idx]\n\nprint(f\"Best threshold = {best_thresh}\")\nprint(f\"Precision = {best_precision}\")\nprint(f\"Recall = {best_recall}\")\nprint(f\"F1 = {best_f1}\")\n\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label='Precision-Recall curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend(loc='best')\n# plot threshold point (max F1)\nplt.scatter(recall[best_idx], precision[best_idx], color='red', label=f'Best Threshold = {best_thresh} (F1 = {best_f1})')\nplt.legend(loc='best')\nplt.savefig(\"precision-recall_curve.png\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}