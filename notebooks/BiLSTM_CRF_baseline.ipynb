{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### BiLSTM-CRF Baseline Tagger\n",
        "\n",
        "This script runs the NER Tagger (https://github.com/glample/tagger) on the GutBrainie2025 (https://hereditary.dei.unipd.it/challenges/gutbrainie/2025/) dataset and uses the official GutBrainie2025 Evaluation script to evaluate its performance (cf. https://github.com/MMartinelli-hub/GutBrainIE_2025_Baseline/)."
      ],
      "metadata": {
        "id": "CJgEHrU9MhtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "S8yFIhkXQ1rH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Python 2.7, Theano and Numpy (as required by https://github.com/glample/tagger)"
      ],
      "metadata": {
        "id": "3CtO3ob74pp6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9rxTdK4tOBlo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea913862-bcf1-4c6e-dc02-2519c6b37ab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,718 kB]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,659 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,907 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,200 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,544 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,420 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
            "Fetched 24.1 MB in 8s (3,091 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython2.7 libpython2.7-dev libpython2.7-minimal libpython2.7-stdlib\n",
            "  python2.7-minimal\n",
            "Suggested packages:\n",
            "  python2.7-doc binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython2.7 libpython2.7-dev libpython2.7-minimal libpython2.7-stdlib\n",
            "  python2.7 python2.7-dev python2.7-minimal\n",
            "0 upgraded, 7 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 7,939 kB of archives.\n",
            "After this operation, 32.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libpython2.7-minimal amd64 2.7.18-13ubuntu1.5 [347 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python2.7-minimal amd64 2.7.18-13ubuntu1.5 [1,400 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libpython2.7-stdlib amd64 2.7.18-13ubuntu1.5 [1,977 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libpython2.7 amd64 2.7.18-13ubuntu1.5 [1,164 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libpython2.7-dev amd64 2.7.18-13ubuntu1.5 [2,514 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python2.7 amd64 2.7.18-13ubuntu1.5 [250 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python2.7-dev amd64 2.7.18-13ubuntu1.5 [287 kB]\n",
            "Fetched 7,939 kB in 1s (5,617 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 7.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython2.7-minimal:amd64.\n",
            "(Reading database ... 126101 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpython2.7-minimal_2.7.18-13ubuntu1.5_amd64.deb ...\n",
            "Unpacking libpython2.7-minimal:amd64 (2.7.18-13ubuntu1.5) ...\n",
            "Selecting previously unselected package python2.7-minimal.\n",
            "Preparing to unpack .../1-python2.7-minimal_2.7.18-13ubuntu1.5_amd64.deb ...\n",
            "Unpacking python2.7-minimal (2.7.18-13ubuntu1.5) ...\n",
            "Selecting previously unselected package libpython2.7-stdlib:amd64.\n",
            "Preparing to unpack .../2-libpython2.7-stdlib_2.7.18-13ubuntu1.5_amd64.deb ...\n",
            "Unpacking libpython2.7-stdlib:amd64 (2.7.18-13ubuntu1.5) ...\n",
            "Selecting previously unselected package libpython2.7:amd64.\n",
            "Preparing to unpack .../3-libpython2.7_2.7.18-13ubuntu1.5_amd64.deb ...\n",
            "Unpacking libpython2.7:amd64 (2.7.18-13ubuntu1.5) ...\n",
            "Selecting previously unselected package libpython2.7-dev:amd64.\n",
            "Preparing to unpack .../4-libpython2.7-dev_2.7.18-13ubuntu1.5_amd64.deb ...\n",
            "Unpacking libpython2.7-dev:amd64 (2.7.18-13ubuntu1.5) ...\n",
            "Selecting previously unselected package python2.7.\n",
            "Preparing to unpack .../5-python2.7_2.7.18-13ubuntu1.5_amd64.deb ...\n",
            "Unpacking python2.7 (2.7.18-13ubuntu1.5) ...\n",
            "Selecting previously unselected package python2.7-dev.\n",
            "Preparing to unpack .../6-python2.7-dev_2.7.18-13ubuntu1.5_amd64.deb ...\n",
            "Unpacking python2.7-dev (2.7.18-13ubuntu1.5) ...\n",
            "Setting up libpython2.7-minimal:amd64 (2.7.18-13ubuntu1.5) ...\n",
            "Setting up python2.7-minimal (2.7.18-13ubuntu1.5) ...\n",
            "Linking and byte-compiling packages for runtime python2.7...\n",
            "Setting up libpython2.7-stdlib:amd64 (2.7.18-13ubuntu1.5) ...\n",
            "Setting up libpython2.7:amd64 (2.7.18-13ubuntu1.5) ...\n",
            "Setting up libpython2.7-dev:amd64 (2.7.18-13ubuntu1.5) ...\n",
            "Setting up python2.7 (2.7.18-13ubuntu1.5) ...\n",
            "Setting up python2.7-dev (2.7.18-13ubuntu1.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "--2025-05-03 08:13:31--  https://bootstrap.pypa.io/pip/2.7/get-pip.py\n",
            "Resolving bootstrap.pypa.io (bootstrap.pypa.io)... 151.101.0.175, 151.101.64.175, 151.101.128.175, ...\n",
            "Connecting to bootstrap.pypa.io (bootstrap.pypa.io)|151.101.0.175|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1908226 (1.8M) [text/x-python]\n",
            "Saving to: ‘get-pip.py’\n",
            "\n",
            "get-pip.py          100%[===================>]   1.82M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-05-03 08:13:31 (25.0 MB/s) - ‘get-pip.py’ saved [1908226/1908226]\n",
            "\n",
            "Collecting pip<21.0\n",
            "  Downloading pip-20.3.4-py2.py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 3.0 MB/s \n",
            "\u001b[?25hCollecting setuptools<45\n",
            "  Downloading setuptools-44.1.1-py2.py3-none-any.whl (583 kB)\n",
            "\u001b[K     |████████████████████████████████| 583 kB 19.4 MB/s \n",
            "\u001b[?25hCollecting wheel\n",
            "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: pip, setuptools, wheel\n",
            "Successfully installed pip-20.3.4 setuptools-44.1.1 wheel-0.37.1\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.16.6-cp27-cp27mu-manylinux1_x86_64.whl (17.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.0 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting Theano\n",
            "  Downloading Theano-1.0.5.tar.gz (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 45.1 MB/s \n",
            "\u001b[?25hCollecting scipy>=0.14\n",
            "  Downloading scipy-1.2.3-cp27-cp27mu-manylinux1_x86_64.whl (24.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.8 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting six>=1.9.0\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: Theano\n",
            "  Building wheel for Theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Theano: filename=Theano-1.0.5-py2-none-any.whl size=2668093 sha256=222396a245db4b52fce10506c84f342217756be985fda8c0e94388c68970caa0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/c0/8b/7b7a084a2039cebc816b780a71023a5424e5525942432ed906\n",
            "Successfully built Theano\n",
            "Installing collected packages: numpy, scipy, six, Theano\n",
            "Successfully installed Theano-1.0.5 numpy-1.16.6 scipy-1.2.3 six-1.17.0\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install python2.7 python2.7-dev\n",
        "!wget https://bootstrap.pypa.io/pip/2.7/get-pip.py\n",
        "!python2.7 get-pip.py\n",
        "!python2.7 -m pip install numpy Theano # theano needs pygpu, however, versions are not supported and compatible anymore. So we use CPU..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Upload the Baseline model folder and the training and test data from GutBrainie2025"
      ],
      "metadata": {
        "id": "x9c1XKSG4uJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the Baseline folder as zip\n",
        "uploaded = files.upload()\n",
        "!unzip BiLSTM-CRF-NER-Baseline.zip -d BiLSTM-CRF-NER-Baseline"
      ],
      "metadata": {
        "id": "Wc08X3jeKqXh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b413c41d-40e2-4c16-8979-368c41de3875"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-83964a06-18c4-4a3f-ae83-c4a959abf5a7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-83964a06-18c4-4a3f-ae83-c4a959abf5a7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving BiLSTM-CRF-NER-Baseline.zip to BiLSTM-CRF-NER-Baseline.zip\n",
            "Archive:  BiLSTM-CRF-NER-Baseline.zip\n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/._BiLSTM-CRF-NER-Baseline  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/LICENSE.md  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/nn.pyc  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/optimization.pyc  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.DS_Store  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/._.DS_Store  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/get-pip.py  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/loader.pyc  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/nn.py  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/dataset/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/._dataset  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/run_on_ucloud.sh  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/model.pyc  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/optimization.py  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/._models  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/model.py  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/._model.py  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/README.md  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/._README.md  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/tagger.py  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/._tagger.py  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/utils.py  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/._utils.py  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/loader.py  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/train.py  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/._train.py  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.python-version  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/evaluation/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/._evaluation  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/._.git  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/utils.pyc  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/dataset/eng.testa  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/dataset/eng.train  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/dataset/eng.testb  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/.DS_Store  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/models/._.DS_Store  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/english/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/models/._english  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/evaluation/temp/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/evaluation/conlleval  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/evaluation/._conlleval  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/evaluation/.DS_Store  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/evaluation/._.DS_Store  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/config  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/objects/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/._objects  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/HEAD  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/info/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/._info  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/logs/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/._logs  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/description  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/hooks/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/._hooks  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/refs/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/._refs  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/index  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/packed-refs  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/COMMIT_EDITMSG  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/english/word_lstm_rev.mat  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/english/char_lstm_rev.mat  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/english/mappings.pkl  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/english/word_layer.mat  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/english/transitions.mat  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/english/final_layer.mat  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/english/tanh_layer.mat  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/english/parameters.pkl  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/english/word_lstm_for.mat  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/english/char_lstm_for.mat  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/english/char_layer.mat  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/objects/66/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/objects/._66  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/objects/69/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/objects/._69  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/objects/pack/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/objects/._pack  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/objects/info/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/objects/._info  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/objects/cc/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/objects/._cc  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/info/exclude  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/logs/HEAD  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/logs/refs/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/logs/._refs  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/hooks/commit-msg.sample  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/hooks/pre-rebase.sample  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/hooks/pre-commit.sample  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/hooks/pre-receive.sample  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/hooks/post-update.sample  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/hooks/pre-merge-commit.sample  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/hooks/pre-push.sample  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/hooks/update.sample  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/hooks/push-to-checkout.sample  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/refs/heads/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/refs/._heads  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/refs/tags/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/refs/._tags  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/refs/remotes/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/refs/._remotes  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/objects/66/da66222dc6b2ee151e223e58b6ea8dbc36f31a  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/objects/69/9bae71005963f22089474bd89d62f9127db2f9  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/objects/pack/pack-2b9103d5debb0833dcdee77708fb87d567073d4d.pack  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/objects/pack/pack-2b9103d5debb0833dcdee77708fb87d567073d4d.idx  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/objects/cc/8bbf4d66f7293ec1d82d26a140867bf768336d  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/logs/refs/heads/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/logs/refs/._heads  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/logs/refs/remotes/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/logs/refs/._remotes  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/refs/heads/master  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/refs/remotes/origin/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/refs/remotes/._origin  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/logs/refs/heads/master  \n",
            "   creating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/logs/refs/remotes/origin/\n",
            "  inflating: BiLSTM-CRF-NER-Baseline/__MACOSX/BiLSTM-CRF-NER-Baseline/.git/logs/refs/remotes/._origin  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/refs/remotes/origin/HEAD  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/refs/remotes/origin/master  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/logs/refs/remotes/origin/HEAD  \n",
            "  inflating: BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/.git/logs/refs/remotes/origin/master  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the GutBrainIE2025 dataset\n",
        "uploaded = files.upload()\n",
        "!unzip gutbrainie2025.zip -d GutBrainIE2025"
      ],
      "metadata": {
        "id": "63ai8y8HMsRg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e03f1a7-0e8f-4490-8c57-84d50438cb19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-802d0dd6-6524-4152-a851-a67a7b1b2dd0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-802d0dd6-6524-4152-a851-a67a7b1b2dd0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving gutbrainie2025.zip to gutbrainie2025.zip\n",
            "Archive:  gutbrainie2025.zip\n",
            "   creating: GutBrainIE2025/gutbrainie2025/\n",
            "  inflating: GutBrainIE2025/__MACOSX/._gutbrainie2025  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/.DS_Store  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/._.DS_Store  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Articles/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/._Articles  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/._Annotations  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/.DS_Store  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/._.DS_Store  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Articles/csv_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/._csv_format  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Articles/json_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/._json_format  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Articles/txt_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/._txt_format  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/.DS_Store  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/._.DS_Store  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/._Train  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/._Dev  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/csv_format/articles_train_bronze.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/csv_format/._articles_train_bronze.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/csv_format/articles_dev.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/csv_format/._articles_dev.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/csv_format/articles_train_platinum.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/csv_format/._articles_train_platinum.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/csv_format/articles_train_gold.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/csv_format/._articles_train_gold.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/csv_format/articles_train_silver.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/csv_format/._articles_train_silver.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/json_format/articles_train_gold.json  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/json_format/._articles_train_gold.json  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/json_format/articles_train_bronze.json  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/json_format/._articles_train_bronze.json  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/json_format/articles_train_silver.json  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/json_format/._articles_train_silver.json  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/json_format/articles_dev.json  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/json_format/._articles_dev.json  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/json_format/articles_train_platinum.json  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/json_format/._articles_train_platinum.json  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/txt_format/articles_train_gold.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/txt_format/._articles_train_gold.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/txt_format/articles_train_silver.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/txt_format/._articles_train_silver.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/txt_format/articles_train_bronze.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/txt_format/._articles_train_bronze.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/txt_format/articles_dev.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/txt_format/._articles_dev.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Articles/txt_format/articles_train_platinum.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Articles/txt_format/._articles_train_platinum.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/.DS_Store  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/._.DS_Store  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/._bronze_quality  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/._platinum_quality  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/._gold_quality  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/._silver_quality  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/.DS_Store  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/._.DS_Store  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/csv_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/._csv_format  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/json_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/._json_format  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/txt_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/._txt_format  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/.DS_Store  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/._.DS_Store  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/csv_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/._csv_format  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/json_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/._json_format  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/txt_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/._txt_format  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/.DS_Store  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/._.DS_Store  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/csv_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/._csv_format  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/json_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/._json_format  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/txt_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/._txt_format  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/.DS_Store  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/._.DS_Store  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/csv_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/._csv_format  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/json_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/._json_format  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/txt_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/._txt_format  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/.DS_Store  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/._.DS_Store  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/csv_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/._csv_format  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/json_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/._json_format  \n",
            "   creating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/txt_format/\n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/._txt_format  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/csv_format/dev_ternary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/csv_format/._dev_ternary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/csv_format/dev_binary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/csv_format/._dev_binary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/csv_format/dev_ternary_mention_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/csv_format/._dev_ternary_mention_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/csv_format/dev_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/csv_format/._dev_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/csv_format/dev_entities.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/csv_format/._dev_entities.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/json_format/dev.json  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/json_format/._dev.json  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/txt_format/dev_ternary_mention_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/txt_format/._dev_ternary_mention_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/txt_format/dev_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/txt_format/._dev_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/txt_format/dev_entities.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/txt_format/._dev_entities.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/txt_format/dev_ternary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/txt_format/._dev_ternary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Dev/txt_format/dev_binary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Dev/txt_format/._dev_binary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/csv_format/train_bronze_ternary_mention_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/csv_format/._train_bronze_ternary_mention_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/csv_format/train_bronze_binary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/csv_format/._train_bronze_binary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/csv_format/train_bronze_ternary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/csv_format/._train_bronze_ternary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/csv_format/train_bronze_entities.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/csv_format/._train_bronze_entities.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/csv_format/train_bronze_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/csv_format/._train_bronze_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/json_format/train_bronze.json  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/json_format/._train_bronze.json  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/txt_format/train_bronze_ternary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/txt_format/._train_bronze_ternary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/txt_format/train_bronze_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/txt_format/._train_bronze_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/txt_format/train_bronze_entities.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/txt_format/._train_bronze_entities.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/txt_format/train_bronze_ternary_mention_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/txt_format/._train_bronze_ternary_mention_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/bronze_quality/txt_format/train_bronze_binary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/bronze_quality/txt_format/._train_bronze_binary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/csv_format/train_platinum_binary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/csv_format/._train_platinum_binary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/csv_format/train_platinum_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/csv_format/._train_platinum_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/csv_format/train_platinum_ternary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/csv_format/._train_platinum_ternary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/csv_format/train_platinum_ternary_mention_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/csv_format/._train_platinum_ternary_mention_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/csv_format/train_platinum_entities.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/csv_format/._train_platinum_entities.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/json_format/train_platinum.json  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/json_format/._train_platinum.json  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/txt_format/train_platinum_ternary_mention_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/txt_format/._train_platinum_ternary_mention_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/txt_format/train_platinum_entities.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/txt_format/._train_platinum_entities.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/txt_format/train_platinum_binary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/txt_format/._train_platinum_binary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/txt_format/train_platinum_ternary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/txt_format/._train_platinum_ternary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/platinum_quality/txt_format/train_platinum_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/platinum_quality/txt_format/._train_platinum_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/csv_format/train_gold_binary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/csv_format/._train_gold_binary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/csv_format/train_gold_ternary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/csv_format/._train_gold_ternary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/csv_format/train_gold_entities.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/csv_format/._train_gold_entities.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/csv_format/train_gold_ternary_mention_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/csv_format/._train_gold_ternary_mention_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/csv_format/train_gold_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/csv_format/._train_gold_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/json_format/train_gold.json  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/json_format/._train_gold.json  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/txt_format/train_gold_ternary_mention_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/txt_format/._train_gold_ternary_mention_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/txt_format/train_gold_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/txt_format/._train_gold_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/txt_format/train_gold_ternary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/txt_format/._train_gold_ternary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/txt_format/train_gold_binary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/txt_format/._train_gold_binary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/gold_quality/txt_format/train_gold_entities.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/gold_quality/txt_format/._train_gold_entities.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/csv_format/train_silver_ternary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/csv_format/._train_silver_ternary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/csv_format/train_silver_binary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/csv_format/._train_silver_binary_tag_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/csv_format/train_silver_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/csv_format/._train_silver_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/csv_format/train_silver_ternary_mention_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/csv_format/._train_silver_ternary_mention_based_relations.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/csv_format/train_silver_entities.csv  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/csv_format/._train_silver_entities.csv  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/json_format/train_silver.json  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/json_format/._train_silver.json  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/txt_format/train_silver_binary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/txt_format/._train_silver_binary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/txt_format/train_silver_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/txt_format/._train_silver_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/txt_format/train_silver_ternary_mention_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/txt_format/._train_silver_ternary_mention_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/txt_format/train_silver_entities.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/txt_format/._train_silver_entities.txt  \n",
            "  inflating: GutBrainIE2025/gutbrainie2025/Annotations/Train/silver_quality/txt_format/train_silver_ternary_tag_based_relations.txt  \n",
            "  inflating: GutBrainIE2025/__MACOSX/gutbrainie2025/Annotations/Train/silver_quality/txt_format/._train_silver_ternary_tag_based_relations.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!ls -l"
      ],
      "metadata": {
        "id": "vBlV-8L5MyUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72da6d1e-73d5-4e3f-bbd6-5b1154c9fc37"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "total 195364\n",
            "drwxr-xr-x 4 root root      4096 May  3 08:25 BiLSTM-CRF-NER-Baseline\n",
            "-rw-r--r-- 1 root root 189783562 May  3 08:25 BiLSTM-CRF-NER-Baseline.zip\n",
            "-rw-r--r-- 1 root root   1908226 May  2 15:47 get-pip.py\n",
            "drwxr-xr-x 4 root root      4096 May  3 08:30 GutBrainIE2025\n",
            "-rw-r--r-- 1 root root   8339467 May  3 08:30 gutbrainie2025.zip\n",
            "drwxr-xr-x 1 root root      4096 Apr 30 13:37 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocess Training, Validation, and Test data for the model"
      ],
      "metadata": {
        "id": "hv02PfjL4R4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "from pathlib import Path\n",
        "import nltk\n",
        "from nltk.tokenize import PunktSentenceTokenizer, TreebankWordTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def read_json_files_from_dir(train_dir):\n",
        "    combined_data = {}\n",
        "    # get all json files excluding bronze quality since they are autogenerated\n",
        "    for json_file in train_dir.glob(\"*/json_format/*.json\"):\n",
        "        if \"bronze\" in str(json_file):\n",
        "            continue\n",
        "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "            # merge the data\n",
        "            combined_data.update(data)\n",
        "    return combined_data\n",
        "\n",
        "def read_json_file(json_path):\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def split_train_dev(train_data, dev_size):\n",
        "    keys = list(train_data.keys())\n",
        "    random.shuffle(keys)\n",
        "    keys = sorted(list(train_data.keys()))\n",
        "    dev_keys = keys[:dev_size]\n",
        "    train_keys = keys[dev_size:]\n",
        "    train_split = {k: train_data[k] for k in train_keys}\n",
        "    dev_split = {k: train_data[k] for k in dev_keys}\n",
        "    return train_split, dev_split\n",
        "\n",
        "\n",
        "def get_bio_labels(tokens, entities):\n",
        "    \"\"\"\n",
        "    For a list of tokens and entity annotations, assigns BIO labels based on the entity spans. The entities should have a start_idx, an end_idx, and a label.\n",
        "    \"\"\"\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "    for entity in entities:\n",
        "        start_idx = entity[\"start_idx\"]\n",
        "        end_idx = entity[\"end_idx\"] + 1  # exclusive\n",
        "        ent_label = entity[\"label\"]\n",
        "        if \" \" in ent_label:\n",
        "          ent_label = ent_label.replace(\" \", \"_\")\n",
        "\n",
        "        # assign labels to all tokens that fall inside the entity span.\n",
        "        for i, (token, t_start, t_end) in enumerate(tokens):\n",
        "            if t_start >= start_idx and t_end <= end_idx:\n",
        "                if t_start == start_idx:\n",
        "                    labels[i] = f\"B-{ent_label}\"\n",
        "                else:\n",
        "                    labels[i] = f\"I-{ent_label}\"\n",
        "    return labels\n",
        "\n",
        "def process_text_field(article, location):\n",
        "    \"\"\"\n",
        "    Processes one text field (e.g. \"title\" or \"abstract\") from an article ID. First, the text is tokenized into sentences, and then to words. Then, the token spans are adjusted to absolute offsets, so they can be used for BIO tagging.\n",
        "    Returns a list of tokens and their labels (as a tuple).\n",
        "\n",
        "    \"\"\"\n",
        "    if \"metadata\" not in article or not article[\"metadata\"].get(location):\n",
        "        return []\n",
        "\n",
        "    text = article[\"metadata\"][location]\n",
        "\n",
        "    sentence_tokenizer = PunktSentenceTokenizer()\n",
        "    sentence_spans = list(sentence_tokenizer.span_tokenize(text)) # get sentence spans\n",
        "\n",
        "    treebank_tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "    sentences_token_labels = []\n",
        "\n",
        "    for sent_start, sent_end in sentence_spans: # iterate through the sentences\n",
        "        sentence_text = text[sent_start:sent_end]\n",
        "        tokens = []\n",
        "        # tokenize the sentence and get spans\n",
        "        for token_start_rel, token_end_rel in treebank_tokenizer.span_tokenize(sentence_text):\n",
        "            token = sentence_text[token_start_rel:token_end_rel]\n",
        "            # adjust the relative offsets to the absolute offsets by adding sent_start\n",
        "            abs_token_start = sent_start + token_start_rel\n",
        "            abs_token_end = sent_start + token_end_rel\n",
        "            tokens.append((token, abs_token_start, abs_token_end))\n",
        "\n",
        "        entities = [ent for ent in article.get(\"entities\", []) if ent.get(\"location\") == location]\n",
        "        bio_labels = get_bio_labels(tokens, entities)\n",
        "        token_label_pairs = [(token, label) for (token, _, _), label in zip(tokens, bio_labels)]\n",
        "        sentences_token_labels.append(token_label_pairs)\n",
        "\n",
        "    return sentences_token_labels\n",
        "\n",
        "\n",
        "def write_output(data, output_path):\n",
        "    \"\"\"\n",
        "    Writes tokenized output to a file and makes sure to insert an empty line between each sentence to match the ouput required by Lample.\n",
        "    \"\"\"\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for article_id, article in data.items():\n",
        "            for location in [\"title\", \"abstract\"]:\n",
        "                sentences = process_text_field(article, location)\n",
        "                for sentence in sentences:\n",
        "                    for token, label in sentence:\n",
        "                        f.write(f\"{token}\\t{label}\\n\")\n",
        "                    # add empty line after each sentence\n",
        "                    f.write(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "TRAIN_DIR = Path(\"/content/GutBrainIE2025/gutbrainie2025/Annotations/Train\")\n",
        "PATH_TEST = \"GutBrainIE2025/gutbrainie2025/Annotations/Dev/json_format/dev.json\"\n",
        "\n",
        "train_data = read_json_files_from_dir(TRAIN_DIR)\n",
        "test_data = read_json_file(PATH_TEST)\n",
        "\n",
        "dev_size = len(test_data) # dev size should be the same as test size\n",
        "new_train, new_dev = split_train_dev(train_data, dev_size)\n",
        "\n",
        "write_output(new_train, \"train.txt\")\n",
        "write_output(new_dev, \"dev.txt\")\n",
        "write_output(test_data, \"test.txt\")\n"
      ],
      "metadata": {
        "id": "W3FCka50sIFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36cb7a45-44fa-48e5-c5e0-ee75ce218a87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model in IOB tag scheme"
      ],
      "metadata": {
        "id": "pDLVLihK4cMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#files.download(\"train.txt\")\n",
        "#files.download(\"dev.txt\")\n",
        "#files.download(\"test.txt\") # sanity check\n",
        "%cd /content/BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/\n",
        "!python2.7 train.py --train /content/train.txt --dev /content/dev.txt --test /content/test.txt --tag_scheme iob # train the model with IOB tagging scheme"
      ],
      "metadata": {
        "id": "D_3EgWi4M8gL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31237511-53e1-4750-b927-48efc8ae607f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline\n",
            "Model location: ./models/tag_scheme=iob,lower=False,zeros=False,char_dim=25,char_lstm_dim=25,char_bidirect=True,word_dim=100,word_lstm_dim=100,word_bidirect=True,pre_emb=,all_emb=False,cap_dim=0,crf=True,dropout=0.5,lr_method=sgd-lr_.005\n",
            "Found 14597 unique words (223177 in total)\n",
            "Found 125 unique characters\n",
            "Found 27 unique named entity tags\n",
            "8478 / 408 / 432 sentences in train / dev / test.\n",
            "Saving the mappings to disk...\n",
            "Compiling...\n",
            "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
            "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
            "Starting epoch 0...\n",
            "Score on dev: 48.92000\n",
            "Score on test: 42.62000\n",
            "New best score on dev.\n",
            "Saving model to disk...\n",
            "New best score on test.\n",
            "Epoch 0 done. Average cost: 15.376952\n",
            "Starting epoch 1...\n",
            "Score on dev: 65.28000\n",
            "Score on test: 60.09000\n",
            "New best score on dev.\n",
            "Saving model to disk...\n",
            "New best score on test.\n",
            "Epoch 1 done. Average cost: 8.356228\n",
            "Starting epoch 2...\n",
            "Score on dev: 65.64000\n",
            "Score on test: 65.08000\n",
            "New best score on dev.\n",
            "Saving model to disk...\n",
            "New best score on test.\n",
            "Epoch 2 done. Average cost: 6.291449\n",
            "Starting epoch 3...\n",
            "Score on dev: 73.16000\n",
            "Score on test: 70.72000\n",
            "New best score on dev.\n",
            "Saving model to disk...\n",
            "New best score on test.\n",
            "Epoch 3 done. Average cost: 5.174123\n",
            "Starting epoch 4...\n",
            "Score on dev: 73.05000\n",
            "Score on test: 70.29000\n",
            "Epoch 4 done. Average cost: 4.608457\n",
            "Starting epoch 5...\n",
            "Score on dev: 73.87000\n",
            "Score on test: 69.35000\n",
            "New best score on dev.\n",
            "Saving model to disk...\n",
            "Epoch 5 done. Average cost: 4.004349\n",
            "Starting epoch 6...\n",
            "Score on dev: 76.52000\n",
            "Score on test: 70.20000\n",
            "New best score on dev.\n",
            "Saving model to disk...\n",
            "Epoch 6 done. Average cost: 3.673044\n",
            "Starting epoch 7...\n",
            "Score on dev: 74.89000\n",
            "Score on test: 71.44000\n",
            "New best score on test.\n",
            "Epoch 7 done. Average cost: 3.394342\n",
            "Starting epoch 8...\n",
            "Score on dev: 74.57000\n",
            "Score on test: 70.90000\n",
            "Epoch 8 done. Average cost: 3.168600\n",
            "Starting epoch 9...\n",
            "Score on dev: 73.53000\n",
            "Score on test: 71.30000\n",
            "Epoch 9 done. Average cost: 2.966886\n",
            "Starting epoch 10...\n",
            "Score on dev: 75.39000\n",
            "Score on test: 73.68000\n",
            "New best score on test.\n",
            "Epoch 10 done. Average cost: 2.847782\n",
            "Starting epoch 11...\n",
            "Score on dev: 75.34000\n",
            "Score on test: 72.22000\n",
            "Early stopping at epoch 12\n",
            "Training losses per epoch:\n",
            "[15.376951570901342, 8.356227606169387, 6.291449376786945, 5.174122606555074, 4.608457359911772, 4.004348555086518, 3.6730437050478524, 3.3943418329697828, 3.168599850739309, 2.9668855133056433, 2.8477819981876227]\n",
            "Development F1 scores per epoch:\n",
            "[48.92, 65.28, 65.64, 73.16, 73.05, 73.87, 76.52, 74.89, 74.57, 73.53, 75.39]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content # rename the default model name so we can use it more easily\n",
        "!mv 'BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/tag_scheme=iob,lower=False,zeros=False,char_dim=25,char_lstm_dim=25,char_bidirect=True,word_dim=100,word_lstm_dim=100,word_bidirect=True,pre_emb=,all_emb=False,cap_dim=0,crf=True,dropout=0.5,lr_method=sgd-lr_.005' 'BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/GutBrainiemodelIOB'\n",
        "\n",
        "!ls {'BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/'}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KBT0POHlpRo",
        "outputId": "49796f0f-4acf-421b-c3d0-057d91b99808"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "english  GutBrainiemodelIOB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {'BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/'}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qd2eXSCk_g4",
        "outputId": "5dd73e1a-837b-463c-c5cb-61bbbe92e395"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "english  GutBrainiemodelIOB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tagging entities with the trained model\n",
        "#### Step 1: Prepare input files for the tagger and subsequently tag them"
      ],
      "metadata": {
        "id": "VdZ_DH2b1vZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "import nltk\n",
        "from nltk.tokenize import PunktSentenceTokenizer, TreebankWordTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "TEST_JSON_PATH = \"GutBrainIE2025/gutbrainie2025/Annotations/Dev/json_format/dev.json\"\n",
        "TAGGER_INPUT_DIR = \"tagger_input\"\n",
        "TAGGER_OUTPUT_DIR = \"tagger_output\"\n",
        "FINAL_OUTPUT_FILE = \"tagged_test.json\"\n",
        "\n",
        "TAGGER_SCRIPT = \"BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/tagger.py\"\n",
        "TAGGER_MODEL = \"BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/GutBrainiemodelIOB/\"\n",
        "\n",
        "\n",
        "def load_test_data(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def prepare_tagger_input(test_data, output_dir):\n",
        "    \"\"\"\n",
        "    For each article in test_data, for each location (title and abstract),\n",
        "    create an input file for the tagger. Each file will have one sentence per line,\n",
        "    where sentences are tokenized using the TreebankWordTokenizer. Them, it saves the output file in output_dir.\n",
        "    \"\"\"\n",
        "    sentence_tokenizer = PunktSentenceTokenizer()\n",
        "    treebank_tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "    for article_id, article in test_data.items():\n",
        "        for location in [\"title\", \"abstract\"]:\n",
        "            if \"metadata\" not in article or not article[\"metadata\"].get(location):\n",
        "                continue\n",
        "            text = article[\"metadata\"][location] # get the text\n",
        "\n",
        "            sentence_spans = list(sentence_tokenizer.span_tokenize(text)) # get the sentence spans\n",
        "            sentences = []\n",
        "            for start, end in sentence_spans:\n",
        "                sentence_text = text[start:end]\n",
        "                tokens = treebank_tokenizer.tokenize(sentence_text) # tokenize sentences\n",
        "                sentence_line = \" \".join(tokens)\n",
        "                sentences.append(sentence_line)\n",
        "\n",
        "            # write the sentences to a file\n",
        "            out_file = os.path.join(output_dir, \"{}_{}.txt\".format(article_id, location))\n",
        "            with open(out_file, \"w\") as f:\n",
        "                for sentence in sentences:\n",
        "                    f.write(sentence + \"\\n\")\n",
        "        #print(\"Done with preparing the input data for the tagger!\")\n",
        "\n",
        "\n",
        "def run_tagger_on_all_inputs():\n",
        "    \"\"\"\n",
        "    Runs the tagger on all input files in TAGGER_INPUT_DIR and saves outputs in TAGGER_OUTPUT_DIR.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(TAGGER_OUTPUT_DIR):\n",
        "        os.makedirs(TAGGER_OUTPUT_DIR)\n",
        "\n",
        "    input_files = [f for f in os.listdir(TAGGER_INPUT_DIR) if f.endswith(\".txt\")]\n",
        "\n",
        "    for input_file in input_files:\n",
        "        input_path = os.path.join(TAGGER_INPUT_DIR, input_file)\n",
        "        output_path = os.path.join(TAGGER_OUTPUT_DIR, input_file)\n",
        "\n",
        "        print(f\"Processing: {input_file} → {output_path}\")\n",
        "\n",
        "        # run the tagger on the test set to get our predictions\n",
        "        subprocess.run([\n",
        "            \"python2.7\", TAGGER_SCRIPT,\n",
        "            \"--model\", TAGGER_MODEL,\n",
        "            \"--input\", input_path,\n",
        "            \"--output\", output_path\n",
        "        ], check=True)\n",
        "\n",
        "    print(f\"Done with tagging.\")\n",
        "\n",
        "\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "test_data = load_test_data(TEST_JSON_PATH)\n",
        "\n",
        "if not os.path.exists(TAGGER_INPUT_DIR):\n",
        "    os.makedirs(TAGGER_INPUT_DIR)\n",
        "\n",
        "prepare_tagger_input(test_data, TAGGER_INPUT_DIR) # prepare the input for the tagger\n",
        "\n",
        "run_tagger_on_all_inputs() # run tagger on all input files and get output files"
      ],
      "metadata": {
        "id": "EaZVsRJ5k5Q9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383bb87d-2c9e-49a0-fdfe-b327500144ba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: 37676767_title.txt → tagger_output/37676767_title.txt\n",
            "Processing: 36978911_title.txt → tagger_output/36978911_title.txt\n",
            "Processing: 30405455_title.txt → tagger_output/30405455_title.txt\n",
            "Processing: 37212075_title.txt → tagger_output/37212075_title.txt\n",
            "Processing: 33968794_abstract.txt → tagger_output/33968794_abstract.txt\n",
            "Processing: 33963281_abstract.txt → tagger_output/33963281_abstract.txt\n",
            "Processing: 32999308_title.txt → tagger_output/32999308_title.txt\n",
            "Processing: 37071196_title.txt → tagger_output/37071196_title.txt\n",
            "Processing: 36532064_title.txt → tagger_output/36532064_title.txt\n",
            "Processing: 33777957_title.txt → tagger_output/33777957_title.txt\n",
            "Processing: 33327540_title.txt → tagger_output/33327540_title.txt\n",
            "Processing: 32438623_title.txt → tagger_output/32438623_title.txt\n",
            "Processing: 30309367_abstract.txt → tagger_output/30309367_abstract.txt\n",
            "Processing: 34290352_abstract.txt → tagger_output/34290352_abstract.txt\n",
            "Processing: 38613087_title.txt → tagger_output/38613087_title.txt\n",
            "Processing: 34480631_abstract.txt → tagger_output/34480631_abstract.txt\n",
            "Processing: 37403161_title.txt → tagger_output/37403161_title.txt\n",
            "Processing: 34480631_title.txt → tagger_output/34480631_title.txt\n",
            "Processing: 33195226_title.txt → tagger_output/33195226_title.txt\n",
            "Processing: 33963281_title.txt → tagger_output/33963281_title.txt\n",
            "Processing: 32041265_abstract.txt → tagger_output/32041265_abstract.txt\n",
            "Processing: 25911232_title.txt → tagger_output/25911232_title.txt\n",
            "Processing: 32389857_abstract.txt → tagger_output/32389857_abstract.txt\n",
            "Processing: 35674870_title.txt → tagger_output/35674870_title.txt\n",
            "Processing: 37212075_abstract.txt → tagger_output/37212075_abstract.txt\n",
            "Processing: 37403161_abstract.txt → tagger_output/37403161_abstract.txt\n",
            "Processing: 35674870_abstract.txt → tagger_output/35674870_abstract.txt\n",
            "Processing: 38203207_abstract.txt → tagger_output/38203207_abstract.txt\n",
            "Processing: 30309367_title.txt → tagger_output/30309367_title.txt\n",
            "Processing: 33777957_abstract.txt → tagger_output/33777957_abstract.txt\n",
            "Processing: 38613087_abstract.txt → tagger_output/38613087_abstract.txt\n",
            "Processing: 32389857_title.txt → tagger_output/32389857_title.txt\n",
            "Processing: 37577447_abstract.txt → tagger_output/37577447_abstract.txt\n",
            "Processing: 37849234_abstract.txt → tagger_output/37849234_abstract.txt\n",
            "Processing: 38525261_title.txt → tagger_output/38525261_title.txt\n",
            "Processing: 25869281_title.txt → tagger_output/25869281_title.txt\n",
            "Processing: 37676767_abstract.txt → tagger_output/37676767_abstract.txt\n",
            "Processing: 36978911_abstract.txt → tagger_output/36978911_abstract.txt\n",
            "Processing: 25940667_abstract.txt → tagger_output/25940667_abstract.txt\n",
            "Processing: 33968794_title.txt → tagger_output/33968794_title.txt\n",
            "Processing: 33271426_abstract.txt → tagger_output/33271426_abstract.txt\n",
            "Processing: 37071196_abstract.txt → tagger_output/37071196_abstract.txt\n",
            "Processing: 31610228_title.txt → tagger_output/31610228_title.txt\n",
            "Processing: 32999308_abstract.txt → tagger_output/32999308_abstract.txt\n",
            "Processing: 33327540_abstract.txt → tagger_output/33327540_abstract.txt\n",
            "Processing: 26923630_abstract.txt → tagger_output/26923630_abstract.txt\n",
            "Processing: 38525261_abstract.txt → tagger_output/38525261_abstract.txt\n",
            "Processing: 28716445_title.txt → tagger_output/28716445_title.txt\n",
            "Processing: 25940667_title.txt → tagger_output/25940667_title.txt\n",
            "Processing: 34205336_abstract.txt → tagger_output/34205336_abstract.txt\n",
            "Processing: 37849234_title.txt → tagger_output/37849234_title.txt\n",
            "Processing: 26923630_title.txt → tagger_output/26923630_title.txt\n",
            "Processing: 32438623_abstract.txt → tagger_output/32438623_abstract.txt\n",
            "Processing: 25911232_abstract.txt → tagger_output/25911232_abstract.txt\n",
            "Processing: 28716445_abstract.txt → tagger_output/28716445_abstract.txt\n",
            "Processing: 34174901_title.txt → tagger_output/34174901_title.txt\n",
            "Processing: 34560239_title.txt → tagger_output/34560239_title.txt\n",
            "Processing: 31610228_abstract.txt → tagger_output/31610228_abstract.txt\n",
            "Processing: 38203207_title.txt → tagger_output/38203207_title.txt\n",
            "Processing: 25869281_abstract.txt → tagger_output/25869281_abstract.txt\n",
            "Processing: 34174901_abstract.txt → tagger_output/34174901_abstract.txt\n",
            "Processing: 28556833_title.txt → tagger_output/28556833_title.txt\n",
            "Processing: 30405455_abstract.txt → tagger_output/30405455_abstract.txt\n",
            "Processing: 30900006_abstract.txt → tagger_output/30900006_abstract.txt\n",
            "Processing: 34205336_title.txt → tagger_output/34205336_title.txt\n",
            "Processing: 34560239_abstract.txt → tagger_output/34560239_abstract.txt\n",
            "Processing: 32041265_title.txt → tagger_output/32041265_title.txt\n",
            "Processing: 33195226_abstract.txt → tagger_output/33195226_abstract.txt\n",
            "Processing: 36804535_title.txt → tagger_output/36804535_title.txt\n",
            "Processing: 28556833_abstract.txt → tagger_output/28556833_abstract.txt\n",
            "Processing: 36804535_abstract.txt → tagger_output/36804535_abstract.txt\n",
            "Processing: 31530799_abstract.txt → tagger_output/31530799_abstract.txt\n",
            "Processing: 31662903_abstract.txt → tagger_output/31662903_abstract.txt\n",
            "Processing: 31662903_title.txt → tagger_output/31662903_title.txt\n",
            "Processing: 30900006_title.txt → tagger_output/30900006_title.txt\n",
            "Processing: 36532064_abstract.txt → tagger_output/36532064_abstract.txt\n",
            "Processing: 31530799_title.txt → tagger_output/31530799_title.txt\n",
            "Processing: 34290352_title.txt → tagger_output/34290352_title.txt\n",
            "Processing: 33271426_title.txt → tagger_output/33271426_title.txt\n",
            "Processing: 37577447_title.txt → tagger_output/37577447_title.txt\n",
            "Done with tagging.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Transform the tagger output to a json file containing the entities\n",
        "\n",
        "This json file will be used for evaluation to compare the baseline to the BERT-based models. The file will be in the required submission format for the GutBrainie2025 challenge (https://hereditary.dei.unipd.it/challenges/gutbrainie/2025/)."
      ],
      "metadata": {
        "id": "dgTszW161Z1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import PunktSentenceTokenizer, TreebankWordTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentence_tokenizer = PunktSentenceTokenizer()\n",
        "word_tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "def get_original_token_spans(text):\n",
        "    \"\"\"\n",
        "    Tokenizes text in the same way as the tagger input is created to ensure proper matching with original text to get the entity spans.\n",
        "    \"\"\"\n",
        "    spans = []\n",
        "\n",
        "    sentence_spans = list(sentence_tokenizer.span_tokenize(text)) # sentence tokenization\n",
        "\n",
        "    for sent_start, sent_end in sentence_spans:\n",
        "        sentence = text[sent_start:sent_end]\n",
        "        word_spans = list(word_tokenizer.span_tokenize(sentence))  # word tokenization\n",
        "\n",
        "        for word_start, word_end in word_spans:\n",
        "            actual_start = sent_start + word_start\n",
        "            actual_end = sent_start + word_end\n",
        "            spans.append((text[actual_start:actual_end], actual_start, actual_end))\n",
        "\n",
        "    return spans\n",
        "\n",
        "def process_tagger_output_file(article_id, location, original_text, tagger_output_file):\n",
        "    \"\"\"\n",
        "    Process the tagger output, aligning tokens to original text and extracting entities based on BIO tagging.\n",
        "    \"\"\"\n",
        "    original_tokens = get_original_token_spans(original_text) # tokenize and get spans from the original or ground truth text\n",
        "\n",
        "    # read the tagger outputs and store list of (token, tag)\n",
        "    tagged_tokens = []\n",
        "    with open(tagger_output_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "\n",
        "    tagged_text = \" \".join(line.strip() for line in lines)  # join lines into a single string (so we can match it better with the original text)\n",
        "    for pair in tagged_text.split():\n",
        "        if \"__\" in pair:\n",
        "            token, label = pair.rsplit(\"__\", 1) # split the token and its label (e.g. gut_B-microbiome)\n",
        "            tagged_tokens.append((token, label))\n",
        "        else:\n",
        "            print(f\"Not valid labeling detected.\")\n",
        "\n",
        "\n",
        "    entities = []\n",
        "    current_entity = None\n",
        "    original_idx = 0  # pointer to track original text tokens\n",
        "\n",
        "    for tagged_token, tag in tagged_tokens:\n",
        "        # find the corresponding token in the original tokenized list\n",
        "        while original_idx < len(original_tokens) and original_tokens[original_idx][0] != tagged_token:\n",
        "            original_idx += 1  # move pointer until we find a match\n",
        "\n",
        "        if original_idx >= len(original_tokens):\n",
        "            continue\n",
        "\n",
        "        orig_token, orig_start, orig_end = original_tokens[original_idx]\n",
        "\n",
        "        if tag.startswith(\"B-\"):\n",
        "            # start a new entity (because B is the prefix for the beginning of an enitity)\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)  # save the previous entity\n",
        "\n",
        "            current_entity = {\n",
        "                \"start_idx\": orig_start,\n",
        "                \"end_idx\": orig_end - 1,  # end offset as inclusive index (in the challenge the index is inclusive)\n",
        "                \"location\": location,\n",
        "                \"text_span\": orig_token,\n",
        "                \"label\": tag[2:]  # remove \"B-\"\n",
        "            }\n",
        "\n",
        "        elif tag.startswith(\"I-\") and current_entity:\n",
        "            # continue the current entity if it starts with I (Inside)\n",
        "            current_entity[\"end_idx\"] = orig_end - 1\n",
        "            current_entity[\"text_span\"] += \" \" + orig_token\n",
        "\n",
        "        elif tag == \"O\" and current_entity:\n",
        "            # end the current entity\n",
        "            entities.append(current_entity)\n",
        "            current_entity = None\n",
        "\n",
        "        original_idx += 1\n",
        "\n",
        "    if current_entity:\n",
        "        entities.append(current_entity)\n",
        "\n",
        "    return entities\n",
        "\n",
        "def process_tagger_outputs(test_data, tagger_output_dir, final_output_file):\n",
        "    \"\"\"\n",
        "    Iterates over each article in test_data, reads the corresponding tagger output files\n",
        "    (for title and abstract each), processes them to extract entity offsets, and writes the\n",
        "    final results into a JSON file.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for article_id, article in test_data.items():\n",
        "        article_entities = []\n",
        "        for location in [\"title\", \"abstract\"]:\n",
        "            if \"metadata\" not in article or not article[\"metadata\"].get(location):\n",
        "                continue\n",
        "            original_text = article[\"metadata\"][location]\n",
        "            tagger_file = os.path.join(tagger_output_dir, \"{}_{}.txt\".format(article_id, location))\n",
        "            if not os.path.exists(tagger_file):\n",
        "                print(\"Tagger output file {} does not exist; skipping.\".format(tagger_file))\n",
        "                continue\n",
        "            entities = process_tagger_output_file(article_id, location, original_text, tagger_file)\n",
        "            article_entities.extend(entities)\n",
        "        results[article_id] = {\"entities\": article_entities}\n",
        "\n",
        "    with open(final_output_file, \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "\n",
        "process_tagger_outputs(test_data, TAGGER_OUTPUT_DIR, FINAL_OUTPUT_FILE)\n",
        "files.download(\"tagged_test.json\") # download the final entities in the test set"
      ],
      "metadata": {
        "id": "Y1WlvC7p027n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f85f2a25-401a-444d-e530-01882bcff62f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_29f0ad7d-dcef-4dd7-8c7a-851591409dcb\", \"tagged_test.json\", 159711)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run the evaluation script from the challenge (cf. https://github.com/MMartinelli-hub/GutBrainIE_2025_Baseline)"
      ],
      "metadata": {
        "id": "K9oX10xqN499"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_underscores_in_labels(data):\n",
        "    \"\"\"\n",
        "    Replaces \"_\" with \" \" in the label section of an entity in a json file.\n",
        "    \"\"\"\n",
        "    for article_id, article_data in data.items():\n",
        "        for entity in article_data.get(\"entities\", []):\n",
        "            if \"label\" in entity:\n",
        "                entity[\"label\"] = entity[\"label\"].replace(\"_\", \" \")\n",
        "\n",
        "    return data\n",
        "\n",
        "def modify_json_file(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Replaces \"_\" with \" \" in a json file and saves the modified json in a specified path.\n",
        "    \"\"\"\n",
        "    with open(input_file, \"r\") as infile:\n",
        "        data = json.load(infile)\n",
        "\n",
        "    updated_data = replace_underscores_in_labels(data)\n",
        "\n",
        "    with open(output_file, \"w\") as outfile:\n",
        "        json.dump(updated_data, outfile, indent=2)\n",
        "    print(f\"Updated JSON saved to {output_file}\")\n",
        "\n",
        "\n",
        "input_file = \"tagged_test.json\"\n",
        "output_file = \"tagged_test_valid_labels.json\"\n",
        "\n",
        "\n",
        "modify_json_file(input_file, output_file) # replace _ with \" \" in the entity labels of the json file\n",
        "\n",
        "# From here the code is taken from the evaluation of the challenge (see https://github.com/MMartinelli-hub/GutBrainIE_2025_Baseline)\n",
        "\n",
        "PREDICTIONS_PATH_6_1 = \"tagged_test_valid_labels.json\"\n",
        "\n",
        "# DEFINE HERE FOR WHICH SUBTASK(S) YOU WANT TO EVAL YOUR PREDICTIONS\n",
        "eval_6_1_NER = True\n",
        "\n",
        "\n",
        "GROUND_TRUTH_PATH =  \"GutBrainIE2025/gutbrainie2025/Annotations/Dev/json_format/dev.json\"\n",
        "\n",
        "try:\n",
        "    with open(GROUND_TRUTH_PATH, 'r', encoding='utf-8') as file:\n",
        "        ground_truth = json.load(file)\n",
        "except OSError:\n",
        "    raise OSError(f'Error in opening the specified json file: {GROUND_TRUTH_PATH}')\n",
        "\n",
        "LEGAL_ENTITY_LABELS = [\n",
        "    \"anatomical location\",\n",
        "    \"animal\",\n",
        "    \"bacteria\",\n",
        "    \"biomedical technique\",\n",
        "    \"chemical\",\n",
        "    \"DDF\",\n",
        "    \"dietary supplement\",\n",
        "    \"drug\",\n",
        "    \"food\",\n",
        "    \"gene\",\n",
        "    \"human\",\n",
        "    \"microbiome\",\n",
        "    \"statistical technique\"\n",
        "]\n",
        "\n",
        "\n",
        "def eval_submission_6_1_NER(path):\n",
        "    try:\n",
        "        with open(path, 'r', encoding='utf-8') as file:\n",
        "            predictions = json.load(file)\n",
        "    except OSError:\n",
        "        raise OSError(f'Error in opening the specified json file: {path}')\n",
        "\n",
        "    ground_truth_NER = dict()\n",
        "    count_annotated_entities_per_label = {}\n",
        "\n",
        "    for pmid, article in ground_truth.items():\n",
        "        if pmid not in ground_truth_NER:\n",
        "            ground_truth_NER[pmid] = []\n",
        "        for entity in article['entities']:\n",
        "            start_idx = int(entity[\"start_idx\"])\n",
        "            end_idx = int(entity[\"end_idx\"])\n",
        "            location = str(entity[\"location\"])\n",
        "            text_span = str(entity[\"text_span\"])\n",
        "            label = str(entity[\"label\"])\n",
        "\n",
        "            entry = (start_idx, end_idx, location, text_span, label)\n",
        "            ground_truth_NER[pmid].append(entry)\n",
        "\n",
        "            if label not in count_annotated_entities_per_label:\n",
        "                count_annotated_entities_per_label[label] = 0\n",
        "            count_annotated_entities_per_label[label] += 1\n",
        "\n",
        "    count_predicted_entities_per_label = {label: 0 for label in list(count_annotated_entities_per_label.keys())}\n",
        "    count_true_positives_per_label = {label: 0 for label in list(count_annotated_entities_per_label.keys())}\n",
        "\n",
        "    for pmid in predictions.keys():\n",
        "        try:\n",
        "            entities = predictions[pmid]['entities']\n",
        "        except KeyError:\n",
        "            raise KeyError(f'{pmid} - Not able to find field \\\"entities\\\" within article')\n",
        "\n",
        "        for entity in entities:\n",
        "            try:\n",
        "                start_idx = int(entity[\"start_idx\"])\n",
        "                end_idx = int(entity[\"end_idx\"])\n",
        "                location = str(entity[\"location\"])\n",
        "                text_span = str(entity[\"text_span\"])\n",
        "                label = str(entity[\"label\"])\n",
        "            except KeyError:\n",
        "                raise KeyError(f'{pmid} - Not able to find one or more of the expected fields for entity: {entity}')\n",
        "\n",
        "            if label not in LEGAL_ENTITY_LABELS:\n",
        "                raise NameError(f'{pmid} - Illegal label {label} for entity: {entity}')\n",
        "\n",
        "            if label in count_predicted_entities_per_label:\n",
        "                count_predicted_entities_per_label[label] += 1\n",
        "\n",
        "            entry = (start_idx, end_idx, location, text_span, label)\n",
        "            if entry in ground_truth_NER[pmid]:\n",
        "                count_true_positives_per_label[label] += 1\n",
        "\n",
        "    count_annotated_entities = sum(count_annotated_entities_per_label[label] for label in list(count_annotated_entities_per_label.keys()))\n",
        "    count_predicted_entities = sum(count_predicted_entities_per_label[label] for label in list(count_annotated_entities_per_label.keys()))\n",
        "    count_true_positives = sum(count_true_positives_per_label[label] for label in list(count_annotated_entities_per_label.keys()))\n",
        "\n",
        "    micro_precision = count_true_positives / (count_predicted_entities + 1e-10)\n",
        "    micro_recall = count_true_positives / (count_annotated_entities + 1e-10)\n",
        "    micro_f1 = 2 * ((micro_precision * micro_recall) / (micro_precision + micro_recall + 1e-10))\n",
        "\n",
        "    precision, recall, f1 = 0, 0, 0\n",
        "    n = 0\n",
        "    for label in list(count_annotated_entities_per_label.keys()):\n",
        "        n += 1\n",
        "        current_precision = count_true_positives_per_label[label] / (count_predicted_entities_per_label[label] + 1e-10)\n",
        "        current_recall = count_true_positives_per_label[label] / (count_annotated_entities_per_label[label] + 1e-10)\n",
        "\n",
        "        precision += current_precision\n",
        "        recall += current_recall\n",
        "        f1 += 2 * ((current_precision * current_recall) / (current_precision + current_recall + 1e-10))\n",
        "\n",
        "    precision = precision / n\n",
        "    recall = recall / n\n",
        "    f1 = f1 / n\n",
        "\n",
        "    return precision, recall, f1, micro_precision, micro_recall, micro_f1\n",
        "\n",
        "\n",
        "\n",
        "round_to_decimal_position = 4\n",
        "\n",
        "if eval_6_1_NER:\n",
        "    precision, recall, f1, micro_precision, micro_recall, micro_f1 = eval_submission_6_1_NER(PREDICTIONS_PATH_6_1)\n",
        "    print(\"\\n\\n=== 6_1_NER ===\")\n",
        "    print(f\"Macro-precision: {round(precision, round_to_decimal_position)}\")\n",
        "    print(f\"Macro-recall: {round(recall, round_to_decimal_position)}\")\n",
        "    print(f\"Macro-F1: {round(f1, round_to_decimal_position)}\")\n",
        "    print(f\"Micro-precision: {round(micro_precision, round_to_decimal_position)}\")\n",
        "    print(f\"Micro-recall: {round(micro_recall, round_to_decimal_position)}\")\n",
        "    print(f\"Micro-F1: {round(micro_f1, round_to_decimal_position)}\")"
      ],
      "metadata": {
        "id": "hufrGQ9TNH2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c98dc99-b743-43e1-b4ff-3d09af2e0c42"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated JSON saved to tagged_test_valid_labels.json\n",
            "\n",
            "\n",
            "=== 6_1_NER ===\n",
            "Macro-precision: 0.5476\n",
            "Macro-recall: 0.4655\n",
            "Macro-F1: 0.4921\n",
            "Micro-precision: 0.7122\n",
            "Micro-recall: 0.607\n",
            "Micro-F1: 0.6554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Some sanity checks and downloading the trained model"
      ],
      "metadata": {
        "id": "R7pPIceU6gvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# have a look at one example tagged file\n",
        "files.download(\"tagger_output/28716445_abstract.txt\")"
      ],
      "metadata": {
        "id": "dOgQJBvRlUKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls -l # folder structure. The cwd should be set to \"/content\"."
      ],
      "metadata": {
        "id": "ckUfW4QApZ-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/BiLSTM-CRF-NER-Baseline/BiLSTM-CRF-NER-Baseline/models/GutBrainiemodelIOB"
      ],
      "metadata": {
        "id": "vHF6yYFa26ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_name = './models/'\n",
        "!zip -r models.zip {folder_name}\n",
        "shutil.make_archive(folder_name, 'zip', folder_name)\n",
        "files.download(f'models.zip') # download models"
      ],
      "metadata": {
        "id": "kMGMhK2yMqFS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}